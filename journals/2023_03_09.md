- On [[ChatGPT]]
	- All these natural language models do is "guess" the next word in a string of text. They do this in a stochastic manner, so every prediction yields a slightly different result. To account for this, they use a parameter called "temperature". At one end of the spectrum, the model is 100% deterministic, always providing the most popular word in the distribution of potential matches. At the other end of the spectrum, the model is 100% stochastic, completely random and unpredictable. Depending on the type of responses desired, there exists a "sweet spot" which is usually around 0.7 for essay writing.
	- This is why [[GPT]] is said to have a "mind of its own", but it is still unclear why these models are so effective at imitating human-like responses, even appearing to produce "new knowledge" in some edge cases. What is well-researched so far is that GPT builds semantic knowledge or a map of reality according to how humans see the world in terms of nouns and verbs. Rather than correlating words by their grammatical structure, the model extracts their meaning. For example, "cats purr", so "purr" has a stronger link to "cats" than "dogs".
	- [[Attention]] is a powerful mechanism in [[transformer architecture]] that allows natural language models like GPT to better understand the relationships between words and their meanings. Essentially, attention in this context refers to the model's ability to assign different weights to different input elements, allowing it to selectively focus on the most important elements for a given task. This means that the model can assign more importance to certain words or phrases based on their relevance to the task at hand, resulting in more accurate and contextually appropriate responses.
	- The attention mechanism works by calculating a score for each input element, which represents how relevant that element is to the output. This score is based on the similarity between the input element and the output, as determined by a weighting function. The scores are then normalized and used as weights to compute a weighted sum of the input elements. This weighted sum is passed through a neural network to produce the output. By selectively attending to the most relevant input elements, the model is able to generate more accurate and contextually appropriate responses.
	- The attention mechanism is particularly important in transformer architecture, which is a type of neural network that is used in many natural language processing tasks. Transformers consist of an encoder and a decoder, which work together to transform input sequences into output sequences. The encoder uses self-attention to selectively focus on the most important input elements, while the decoder uses a combination of self-attention and cross-attention to generate the output.
	- Overall, the attention mechanism is a key component of natural language processing and is essential for the development of more accurate and contextually appropriate language models. By allowing the model to selectively focus on the most relevant input elements, attention enables the model to better understand the relationships between words and their meanings, resulting in more coherent and effective responses.
	- Takeaways
		- The GPT model uses a stochastic approach to "guess" the next word in a string of text, with a temperature parameter that can be adjusted to control the level of randomness. The model builds semantic knowledge based on how humans see the world in terms of nouns and verbs, rather than just by grammatical structure.
		- The attention mechanism in [[transformer architecture]] is a powerful tool that enables the model to selectively focus on the most important input elements, resulting in more accurate and contextually appropriate responses. Transformers consist of an encoder and a decoder, which work together to transform input sequences into output sequences.