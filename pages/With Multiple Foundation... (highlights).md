title:: With Multiple Foundation... (highlights)
author:: [[@andyzengtweets on Twitter]]
full-title:: "With Multiple Foundation..."
category:: #tweets
url:: https://twitter.com/andyzengtweets/status/1512089759497269251

- Highlights first synced by [[Readwise]] [[Nov 19th, 2022]]
	- With multiple foundation models “talking to each other”, we can combine commonsense across domains, to do multimodal tasks like zero-shot video Q&A or image captioning, no finetuning needed.
	  
	  Socratic Models:
	  website + code: https://t.co/Zz0kbV5GTQ 
	  paper: https://t.co/NpsW61Ka3s https://t.co/D5630owUt6 ([View Tweet](https://twitter.com/andyzengtweets/status/1512089759497269251))
		- **Note**: Thread
	- From recalling events, to contextual and temporal reasoning – prompting foundation models to engage in guided Socratic discussions enables a variety of new open-ended video Q&A capabilities. https://t.co/myBKsscCH9 ([View Tweet](https://twitter.com/andyzengtweets/status/1512089793559244802))
	- One way to approach video understanding is to turn it into a reading comprehension problem. This turns a classically hard computer vision task into something that we know large language models are good at. https://t.co/QxvIE04YEj ([View Tweet](https://twitter.com/andyzengtweets/status/1512089828090810376))
	- A couple more examples – here’s zero-shot image captioning, with the large language model (LM) and visual-language model (VLM) working together. Code is already open-source for this one: https://t.co/Kq7j9dBXYm https://t.co/9O9jpthnBf ([View Tweet](https://twitter.com/andyzengtweets/status/1512091758817124357))
	- And here’s video-to-text retrieval. The Socratic Models framework makes it easy to add together new modalities (like speech from audio).  In this case we can provide a new zero-shot SoTA, nearing the best finetuned methods. https://t.co/NAnMgf6JyR ([View Tweet](https://twitter.com/andyzengtweets/status/1512091814903394309))
	- In general, we’re excited about Socratic Models – they present new ways to think about how we can tackle new multimodal applications with the existing foundation models that we already have today, without additional finetuning or data collection. 
	  
	  ![](https://pbs.twimg.com/media/FPwHJjlXwAAM81G.png) ([View Tweet](https://twitter.com/andyzengtweets/status/1512091818736947203))
	- This came out of an amazing collaboration between Robotics and AR teams at Google w/ @almostsquare @tek2222 @kchorolab @fedassa @aveekly @ryoo_michael @vikassindhwani @JohnnyChungLee Vincent Vanhoucke @peteflorence ([View Tweet](https://twitter.com/andyzengtweets/status/1512091821245091850))