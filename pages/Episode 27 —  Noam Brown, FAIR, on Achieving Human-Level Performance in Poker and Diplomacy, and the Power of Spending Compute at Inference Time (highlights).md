title:: Episode 27 —  Noam Brown, FAIR, on Achieving Human-Level Performance in Poker and Diplomacy, and the Power of Spending Compute at Inference Time (highlights)
author:: [[Generally Intelligent]]
full-title:: "Episode 27 —  Noam Brown, FAIR, on Achieving Human-Level Performance in Poker and Diplomacy, and the Power of Spending Compute at Inference Time"
category:: #podcasts
url:: https://share.snipd.com/episode/5fbc8049-f505-4ffd-ac55-dd4343da9ab1

- Highlights first synced by [[Readwise]] [[Feb 11th, 2023]]
	- Search in Imprecise Information Games
	  
	  Key takeaways:
	  (* There are a lot of people working on language models for solving war puzzles or other similar kind of like discrete, much more discrete problems., * I could see that as a good playground for the kind of... When you got this like 100,000 X improvement and then you've done the next year kind of just approving the model, like, there was a year between 20, I think you said you did this in 2015, then you spent 26 years doing this and then in 2017 it was the test., * For the rest of 2015 after the competition, I actually was working on another bot for the... We had this like thing called the annual computer poker competition where every year like these bots would play against each other and I was focused on making our bot for that competition. And so I didn't really get to this experiment until the end of 2015. All 2016 I basically spent working on this idea and then we played the competition in like the first week of... We started like the first week of 2017. I wasn't really exploring that many other ideas. I was just looking into more theoretically sound ways of doing search, doing planning in primitive information games.)
	  
	  Transcript:
	  Speaker 2
	  Yeah. Yeah, there are a bunch of people working on like language models for solving war puzzles or other similar kind of like discrete, much more discrete problems. So I could see that as a good playground for the kind of... When you got this like 100,000 X improvement and then you've done the next year kind of just approving the model, like, there was a year between 20, I think you said you did this in 2015, then you spent 26 years doing this and then in 2017 it was the test. Did you just only do this during that year or did you also explore other things?
	  
	  Speaker 1
	  For the rest of 2015 after the competition, I actually was working on another bot for the... We had this like thing called the annual computer poker competition where every year like these bots would play against each other and I was focused on making our bot for that competition. And so I didn't really get to this experiment until the end of 2015. All 2016 I basically spent working on this idea and then we played the competition in like the first week of... We started like the first week of 2017. I wasn't really exploring that many other ideas. I was just looking into more theoretically sound ways of doing search, doing planning in primitive information games. It's actually not trivial to get a search algorithm to work in a game like this. It's really hard to work in a game like poker because the presence of imperfect information really complicates things. There were a lot of questions around how do you do search properly. To be clear, I wasn't the first one to look into search in imperfect information games. There have been prior work on this, though I don't think anybody really appreciated how big of a difference it made. There's like a lot of reasons why doing search in poker was just really annoying. Like if you increase the inference cost, then all of your experiments become much more expensive and poker is already a really noisy game, so you need to play a ton of games to get any signal on whether your techniques are improving things. So I think that plus the theoretical challenge is how do you make a sound search algorithm in an imperfect information game really discouraged a lot of people from pursuing this direction. But I think if they had known how much of a difference it made, they would have just done it anyway. But that's why I think there was this window for me to really push in this direction. What did you learn about how to actually make a sound search algorithm in an imperfect information game? So there were techniques that were unsound that ended up working well in relatively well in practice, though if you really try to apply them to all situations, it could lead to really bad outcomes. It could make you really exploitable. So there's this concept in imperfect information games of exploitability, which is like how much could a worst-case adversary beat you by. These unsound search algorithms in a rock, paper, scissors kind of situation might tell you to just always throw a rock. And that's a really bad strategy in a game like Koker. You want to be balanced and equitable, and they might end up with this really predictable strategy. A lot of the focus of the theoretically sound algorithms is trying to fix that so that you're always playing this balanced, unpredictable strategy. But then that ended up hurting your performance in practice. And so you have to come up with these clever ways of trying to get the best of both, of having it balanced on a predictable strategy, but still ends up doing well against somebody that's not trying to exploit you. Interesting. ([Time 0:17:53](https://share.snipd.com/snip/7d597a11-40f8-470c-b3bb-8874f0d9bf07))