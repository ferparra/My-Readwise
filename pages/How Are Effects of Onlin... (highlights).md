title:: How Are Effects of Onlin... (highlights)
author:: [[@marketsensei on Twitter]]
full-title:: "How Are Effects of Onlin..."
category:: #tweets
url:: https://twitter.com/marketsensei/status/1477381162888839177

- Highlights first synced by [[Readwise]] [[Nov 19th, 2022]]
	- How are effects of online A/B tests distributed? How often are they not significant? Does achieving significance guarantee meaningful business impact?
	  
	  We answer  these questions in our new paper, “False Discovery in A/B Testing”, recently out in Management Science >> ([View Tweet](https://twitter.com/marketsensei/status/1477381162888839177))
		- **Note**: Thread
	- The paper is co-authored with Christophe Van den Bulte and analyzes over 2,700 online A/B tests that were run on the @Optimizely platform by more than 1,300 experimenters.
	  
	  Link to paper: https://t.co/xcolWSvw1P
	  Non paywalled: https://t.co/iF5EsGqpUg
	  >> ([View Tweet](https://twitter.com/marketsensei/status/1477381164797247491))
	- A big draw of the paper is that @Optimizely have graciously allowed us to publish the data we used in the analysis. We hope this would be valuable to other researchers as well.
	  >> ([View Tweet](https://twitter.com/marketsensei/status/1477381166177165314))
	- First, we analyze the effects of all the A/B tests in our data. They are quite small. The median (and average) webpage variations have roughly zero effect on webpage Engagement. 
	  
	  But the distribution is quite long-tailed with some variations showing big effects.
	  >> 
	  
	  ![](https://pbs.twimg.com/media/FICx2hFXEAUYbOz.png) ([View Tweet](https://twitter.com/marketsensei/status/1477381169931075587))
	- We then classify effects into “null” (zero) and “non-null” (pos or neg), to understand how many experiments, on average, have an underlying zero effect.
	  
	  The answer is about 70%.
	  
	  That is, 70% of effects will not show any impact on Engagement compared to a baseline.
	  >> ([View Tweet](https://twitter.com/marketsensei/status/1477381171948441602))
	- The concept of a true null is somewhat subtle (as we explain in the paper), since we often assume that true-nulls don’t exist. 
	  
	  However, as long as we test for a null hypothesis of a true null using significance testing, there is no reason to assume the null cannot be true.
	  >> ([View Tweet](https://twitter.com/marketsensei/status/1477381173173231620))
	- 70% true nulls just means that if one picks a random webpage variation in our data, it has a high chance of yielding no (or very small) impact. 
	  
	  But what about the statistically significant effects? Wouldn’t they guard us from true nulls by having a low false positive rate?
	  >> ([View Tweet](https://twitter.com/marketsensei/status/1477381174343385090))
	- A statistically significant result generated by a true null effect is called a false discovery. Although we often think of the significance threshold we set for hypothesis testing (e.g., alpha=0.05) as the rate of false discoveries, this is not actually what we get.
	  >> ([View Tweet](https://twitter.com/marketsensei/status/1477381175731793921))
	- alpha is the false positive rate (FPR), or 
	  Pr(effect is significant | effect is null). 
	  
	  We care about the opposite, 
	  Pr(effect is null | effect is significant), 
	  which is called the false discovery rate (FDR).
	  >> 
	  
	  ![](https://pbs.twimg.com/media/FICztEUXMAUwvGi.png) ([View Tweet](https://twitter.com/marketsensei/status/1477381178588073986))
	- Our analysis shows that in our data, hypothesis tests conducted with alpha=0.05 yield an FDR of 18%-25%. 
	  
	  Much higher than 5%.
	  
	  That is, about 20% of significant effects chosen for implementation will not generate the business impact that was observed in the experiment.
	  >> 
	  
	  ![](https://pbs.twimg.com/media/FIC0ufhXsAM7WPg.png) ([View Tweet](https://twitter.com/marketsensei/status/1477381181951913986))
	- We use multiple methods to estimate the rate of true nulls and the FDR, but one was particularly fun to learn about, as it was developed to estimate false discoveries in genomewide studies. You can read about it here: https://t.co/nC3H7t7aVT.
	  >> ([View Tweet](https://twitter.com/marketsensei/status/1477381183629631491))
	- Our paper estimates the business costs of these false discoveries, and discusses and tests possible solutions that firms can implement. The details are a bit beyond the scope of this thread, but we hope that the paper with the accompanying data and code will prove useful.
	  >> ([View Tweet](https://twitter.com/marketsensei/status/1477381185055698944))
	- Generally, firms should try to test more radical variations in order to have larger impact on consumer behavior (we call this “swing for the fences”). Most variations will probably not be very impactful, but once in a while an innovation will prove to be very lucrative.
	  (Fin) ([View Tweet](https://twitter.com/marketsensei/status/1477381186544623621))
	- P.S. I didn't go into all details, minutiae, caveats and disclaimers in the paper. There is much more, and I would love for people to read it. ([View Tweet](https://twitter.com/marketsensei/status/1477381187836518404))