title:: Today a 120B Model Calle... (highlights)
author:: [[@DrJimFan on Twitter]]
full-title:: "Today a 120B Model Calle..."
category:: #tweets
url:: https://twitter.com/DrJimFan/status/1592683269866549249

- Highlights first synced by [[Readwise]] [[Nov 19th, 2022]]
	- Today a 120B model called “Galactica” is open-sourced by @paperswithcode. It’s capable of writing math notations, citations, code, chemical formula, DNA, etc. Here’s why I think Galactica is a huge milestone in open foundation models, scientific automation, and responsible AI: 🧵 
	  
	  ![](https://pbs.twimg.com/media/FhpY_jHVsAAlMXP.jpg) ([View Tweet](https://twitter.com/DrJimFan/status/1592683269866549249))
		- **Note**: Thread
	- Large language models have personalities. They are not shaped by the architecture, but by the training data. Models like GPT-3 and OPT are trained on texts scraped from the internet at large, which unfortunately contains lots of irrelevant, misinformed, or toxic contents. 2/🧵 
	  
	  ![](https://pbs.twimg.com/media/FhpY_11VQAEZ4Fc.jpg) ([View Tweet](https://twitter.com/DrJimFan/status/1592683274719354880))
	- In contrast, scientific texts like academic papers are mostly immune from these data plagues. They contain analytical text with a neutral tone, knowledge backed by evidence, and are written by people who wish to inform rather than inflame. A dataset born in the ivory tower. 3/🧵 ([View Tweet](https://twitter.com/DrJimFan/status/1592683277420482560))
	- Galactica capitalizes on the clean nature of such data and collects 106B tokens (!!), > 80% of which are from 48M academic papers. If Stable Diffusion is an intelligent compression of all human art, then Galactica is its counterpart for science. 4/🧵 
	  
	  ![](https://pbs.twimg.com/media/FhpZAPCVUAIc7VZ.jpg) ([View Tweet](https://twitter.com/DrJimFan/status/1592683282092941313))
	- An extra bonus is that Galactica excels at reasoning skills, because not all tokens are created equal. Imagine the human brainpower spent on each word in a math paper, churned out by PhDs in overdrive. Now compare that to the amount of per-word thinking of a Reddit post 😅. 5/🧵 
	  
	  ![](https://pbs.twimg.com/media/FhpZAmPUAAAErgw.jpg) ([View Tweet](https://twitter.com/DrJimFan/status/1592683288204017664))
	- The approach to boost reasoning is quite interesting too. A special token <work> is used to initiate step-by-step reasoning. Other special tokens are also added to process novel text modes like citations and DNA. Reminds me of the meta language that OpenAI Whisper adopts. 6/🧵 
	  
	  ![](https://pbs.twimg.com/media/FhpZA9SVUAAwaum.jpg) 
	  
	  ![](https://pbs.twimg.com/media/FhpZA9QUAAYtBjp.jpg) ([View Tweet](https://twitter.com/DrJimFan/status/1592683294168350720))
	- With the open model weights, we will be able to probe and unveil new capabilities. For example, works like CLIP study inter-modality representations. How about *inter-disciplinary* transfer? Or even better, can the model discover new "dark knowledge" by connecting the dots? 7/🧵 ([View Tweet](https://twitter.com/DrJimFan/status/1592683297028837376))
	- I dream of a robo-scientist that not only browses through all existing knowledge at superhuman speed, but also helps us venture into the uncharted realms of science, serving as our faithful copilot along the way. Galactic is the first step, and the best is yet to come. END/🧵 
	  
	  ![](https://pbs.twimg.com/media/FhpZBXGVIAAOnno.jpg) ([View Tweet](https://twitter.com/DrJimFan/status/1592683300933742594))