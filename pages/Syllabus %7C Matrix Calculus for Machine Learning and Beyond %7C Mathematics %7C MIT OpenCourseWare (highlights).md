title:: Syllabus | Matrix Calculus for Machine Learning and Beyond | Mathematics | MIT OpenCourseWare (highlights)
author:: [[ocw.mit.edu]]
full-title:: "Syllabus | Matrix Calculus for Machine Learning and Beyond | Mathematics | MIT OpenCourseWare"
category:: #articles
url:: https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2022/pages/syllabus/

- Highlights first synced by [[Readwise]] [[Dec 26th, 2022]]
	- Derivatives as linear operators and linear approximation on arbitrary vector spaces: beyond gradients and Jacobians.
	  Derivatives of functions with matrix inputs and/or outputs (e.g. matrix inverses and determinants). Kronecker products and matrix “vectorization.”
	  Derivatives of matrix factorizations (e.g. eigenvalues/SVD) and derivatives with constraints (e.g. orthogonal matrices).
	  Multidimensional chain rules, and the signifance of right-to-left (“forward”) vs. left-to-right (“reverse”) composition. Chain rules on computational graphs (e.g. neural networks).
	  Forward- and reverse-mode manual and automatic multivariate differentiation.
	  Adjoint methods (vJp/pullback rules) for derivatives of solutions of linear, nonlinear, and differential equations.
	  Application to nonlinear root-finding and optimization. Multidimensional Newton and steepest–descent methods.
	  Applications in engineering/scientific optimization and machine learning.
	  Second derivatives, Hessian matrices, quadratic approximations, and quasi-Newton methods.