title:: The GPT-3 Architecture, on a Napkin (highlights)
author:: [[dugas.ch]]
full-title:: "The GPT-3 Architecture, on a Napkin"
category:: #articles
url:: https://dugas.ch/artificial_curiosity/GPT_architecture.html

- Highlights first synced by [[Readwise]] [[Dec 18th, 2022]]
	- ![](https://dugas.ch/artificial_curiosity/GPT_architecture.html/./img/GPT_architecture/GPT1.png) ([View Highlight](https://read.readwise.io/read/01gmhketaq08car7en8vjrchad))
	- ![](https://dugas.ch/artificial_curiosity/GPT_architecture.html/./img/GPT_architecture/Transformer.png) ([View Highlight](https://read.readwise.io/read/01gmhker7sxgsk7vm7qwac3cb7))
	- due to how matrix multiplication works, the embedding function (a.k.a the embedding weight matrix) is applied to each word encoding (a.k.a row in the sequence-encodings matrix) separately. ([View Highlight](https://read.readwise.io/read/01gmhkfffkd9xwxzgbg4a7wn5s))
	- Simply put, the purpose of attention is: for each output in the sequence, predict which input tokens to focus on and how much. ([View Highlight](https://read.readwise.io/read/01gmhkg0sr3fdwpn8tr3bf9yar))
	- ![](https://dugas.ch/artificial_curiosity/GPT_architecture.html/./img/GPT_architecture/fullarch.png) ([View Highlight](https://read.readwise.io/read/01gmhkgeqkd3c6vjhe6f1wnzx5))
		- **Tags**: #[[gpt-3]]