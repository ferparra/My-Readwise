title:: Grokking (highlights)
author:: [[domluna.com]]
full-title:: "Grokking"
category:: #articles
url:: https://www.domluna.com/grokking/

- Highlights first synced by [[Readwise]] [[May 2nd, 2023]]
	- The ["Grokking Paper"](https://arxiv.org/abs/2201.02177) is one of the most head-scratching papers to come out in the neural network space. It explores the phenomenon of a regime change, whereby the model appears, by all indications, to have overfit the data, and that it's only being exacerbated as training progresses. Validation loss is increasing, and validation accuracy is at a standstill. Meanwhile, 100% training accuracy was hit ages ago. But then, all of a sudden, as if a divine entity itself sprinkled fairy dust on the neural network, validation loss begins to decrease and validation accuracy increases. After a while, both accuracies are at 100%. In essence, the neural network is transitioning from a regime of memorization to generalization. ([View Highlight](https://read.readwise.io/read/01gzcnp8dek5k0cc04wv2tkdjf))
		- **Note**: Neural network transitions from memorization to generalization after "fairy dust".