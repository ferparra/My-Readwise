title:: Word2Vec Explained (highlights)
author:: [[towardsdatascience.com]]
full-title:: "Word2Vec Explained"
category:: #articles
url:: https://towardsdatascience.com/word2vec-explained-49c52b4ccb71

- Highlights first synced by [[Readwise]] [[Nov 19th, 2022]]
	- Word embeddings are an integral part of solving many problems in NLP. They depict how humans understand language to a machine. You can imagine them as a vectorized representation of text.
	- Word embeddings is a technique where individual words are transformed into a numerical representation of the word (a vector). Where each word is mapped to one vector, this vector is then learned in a way which resembles a neural network. The vectors try to capture various characteristics of that word with regard to the overall text. These characteristics can include the semantic relationship of the word, definitions, context, etc.