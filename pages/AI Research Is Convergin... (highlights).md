title:: AI Research Is Convergin... (highlights)
author:: [[@sergeykarayev on Twitter]]
full-title:: "AI Research Is Convergin..."
category:: #tweets
url:: https://twitter.com/sergeykarayev/status/1518677260236034050

- Highlights first synced by [[Readwise]] [[Nov 19th, 2022]]
	- AI research is converging on a major finding: language models are a great substrate for all AI applications.
	  
	  This feels like a HUGE deal.
	  
	  Some examples: ([View Tweet](https://twitter.com/sergeykarayev/status/1518677260236034050))
		- **Note**: Thread
	- Ask free-form questions and receive free-form answers about a video.
	  
	  https://t.co/g2xIksKbFa ([View Tweet](https://twitter.com/sergeykarayev/status/1518677262995902466))
	- Receive illustrations from free-form descriptions (DALL-E is combines two different tricks, one of which is a model that embeds text and images into a common space).
	  
	  https://t.co/3qz9UVzqIm ([View Tweet](https://twitter.com/sergeykarayev/status/1518677265889914880))
	- Get working code from a free-form description of a function.
	  
	  And this is from a model that was 95% trained on general language data, not code specifically.
	  
	  https://t.co/hz70c8yVBk ([View Tweet](https://twitter.com/sergeykarayev/status/1518677268863758336))
	- Does this resemble how human cognition happens?
	  
	  My understanding is that the vast majority of human intelligence is not intermediated by language: most processing happens unconsciously, and only the "tip of the iceberg" is in the form of language. ([View Tweet](https://twitter.com/sergeykarayev/status/1518677271870984192))
	- But the vast majority of these large models are probably not dedicated to language either, only the data-interface layers are.
	  
	  This paper from @_kevinlu @adityagrover_ @pabbeel @IMordatch suggests that models learn general computation from language data.
	  
	  https://t.co/cpnxY3Wmfc ([View Tweet](https://twitter.com/sergeykarayev/status/1518677274656092160))
	- @ericjang11 recently proposed that language == generalization and suggests some ideas stemming from that in a nice post.
	  
	  https://t.co/IDMDtlcw6X ([View Tweet](https://twitter.com/sergeykarayev/status/1518677277520789505))
	- @russelljkaplan thinks through the implications of the extreme compute expense of these models combined with their increasing general usefulness.
	  
	  (I don't agree with his prediction that LM vendors will have Apple/FB-like power over customers, though.)
	  
	  https://t.co/vj1WbhUuOM ([View Tweet](https://twitter.com/sergeykarayev/status/1518677280402276352))
	- And notably, we haven't seen a GPT-3 like interface for non-generative vision tasks yet.
	  
	  As a computer vision guy at heart, this is most exciting to imagine. More on that in a future thread. ([View Tweet](https://twitter.com/sergeykarayev/status/1518677283401175040))
	- Here is another recent paper: language-based state representations improve exploration behavior of an RL agent.
	  
	  https://t.co/yg2DfIumQW ([View Tweet](https://twitter.com/sergeykarayev/status/1518776740264304640))
	- A child raised without language was of normal intelligence, able to communicate non-verbally, and eventually learned language well enough to be understood (but without grammar). 
	  
	  ![](https://pbs.twimg.com/media/FRPC1wZVsAAi2Fv.jpg) ([View Tweet](https://twitter.com/sergeykarayev/status/1518776741279326208))
	- My layperson understanding of therapy is that it is still intermediated by language (although maybe not the promising MDMA/Psilocybin types?).
	  
	  Methods like Internal Family Systems are really interesting because you literally talk to parts of yourself...
	  
	  https://t.co/Es8lZcIR6P ([View Tweet](https://twitter.com/sergeykarayev/status/1518776742491529216))
	- Great point that we may be seeing the results that we're seeing because language-based datasets are the largest we currently have.
	  
	  I think it's more than that. Language evolved to represent our world as compactly as possible.
	  
	  https://t.co/MQGwJHD6YX ([View Tweet](https://twitter.com/sergeykarayev/status/1518776744207171585))
	- An "LLM-sized" dataset of images, or videos, probably wouldn't provide as much information as a dataset of language.
	  
	  In fact, there are perfectly functional, intelligent people who report being unable to imagine anything visual.
	  
	  https://t.co/wDcyaHm9SP ([View Tweet](https://twitter.com/sergeykarayev/status/1518776746048253952))