title:: Brief History of RLHF LL... (highlights)
author:: [[@HelloSurgeAI on Twitter]]
full-title:: "Brief History of RLHF LL..."
category:: #tweets
url:: https://twitter.com/HelloSurgeAI/status/1644325388133253120

- Highlights first synced by [[Readwise]] [[Apr 13th, 2023]]
	- Brief History of RLHF LLMs
	  
	  Here are 5 important works to help you learn about RLHF LLMs: ([View Tweet](https://twitter.com/HelloSurgeAI/status/1644325388133253120))
		- **Note**: Thread
	- Fine-Tuning Language Models from Human Preferences
	  
	  This paper presents one of the earlier experiments from OpenAI on RLHF LLMs. It aims to show the application and impact of reward learning on 4 natural language tasks from a capability and safety perspective.… 
	  
	  ![](https://pbs.twimg.com/media/FtHRUzGWwAQZUDa.jpg) ([View Tweet](https://twitter.com/HelloSurgeAI/status/1644325512444100609))
	- Learning to summarize with human feedback
	  
	  This work proposes the use of RLHF for the specific task of text summarization, an important capability of LLMs. Trains models to optimize for human preferences and significantly outperform both human reference summaries and larger… 
	  
	  ![](https://pbs.twimg.com/media/FtHRbbgXgAENpPP.jpg) ([View Tweet](https://twitter.com/HelloSurgeAI/status/1644325637388107776))
	- Sparrow
	  
	  More recently, DeepMind introduced Sparrow which uses RLHF and human judgments to train dialogue agents to be more helpful, correct, and harmless. It also presents an effort to improve factual question answering and make the model more resilient to adversarial probing.… 
	  
	  ![](https://pbs.twimg.com/media/FtHRg8OXsAAvw5d.jpg) ([View Tweet](https://twitter.com/HelloSurgeAI/status/1644325726395457536))
	- ChatGPT
	  
	  ChatGPT needs no introduction but it leans on the learnings from InstructGPT to better align models to human preferences. It’s one of the more capable language models optimized for dialogue.
	  
	  https://t.co/b0W43D3p3K 
	  
	  ![](https://pbs.twimg.com/media/FtHRmm7WcAIP2C3.jpg) ([View Tweet](https://twitter.com/HelloSurgeAI/status/1644325843131412490))
	- Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback
	  
	  This work comes from Anthropic and is one of the more recent papers applying RLHF to LLMs to build assistants with capabilities aligned with helpfulness and safety.
	  
	  https://t.co/CFAtaWDnMB 
	  
	  ![](https://pbs.twimg.com/media/FtHlWtqXgAAEQij.png) ([View Tweet](https://twitter.com/HelloSurgeAI/status/1644347554329493506))
	- From all these papers and other relevant ones, it’s evident that training LLMs with RLHF to align with human preferences is a big deal!
	  
	  To learn more, check out this recent case study on our partnership with Anthropic and how we help advance RLHF LLMs: https://t.co/qtnatCNYs7 ([View Tweet](https://twitter.com/HelloSurgeAI/status/1644347662215282689))
	- If interested, you can also learn more about RLHF from one of our previous posts.
	  
	  We cover all the latest developments in the world of RLHF LLMs. Make sure to follow @HelloSurgeAI for more.
	  
	  https://t.co/6GLbkUHM7P ([View Tweet](https://twitter.com/HelloSurgeAI/status/1644347911084417033))