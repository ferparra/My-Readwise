title:: "Omnigrok: Grokking Beyo... (highlights)
author:: [[@davisblalock on Twitter]]
full-title:: ""Omnigrok: Grokking Beyo..."
category:: #tweets
url:: https://twitter.com/davisblalock/status/1581551158925017088

- Highlights first synced by [[Readwise]] [[Nov 19th, 2022]]
	- "Omnigrok: Grokking Beyond Algorithmic Data"
	  
	  They argue that grokking, rather than being some quirk of algorithmic tasks, is just what happens when your weights start off too large. [1/8] 
	  
	  ![](https://pbs.twimg.com/media/FfLMZSoUUAAb__9.jpg) ([View Tweet](https://twitter.com/davisblalock/status/1581551158925017088))
		- **Note**: Thread
	- By grokking, I of course mean a big, sudden decrease in test error late in training (https://t.co/sMFOl804ao). [2/8] ([View Tweet](https://twitter.com/davisblalock/status/1581551160510468096))
	- With large initial weights and too little regularization to shrink them, it takes a ton of time for the weight norms to get small enough for the model to generalize. [3/8] 
	  
	  ![](https://pbs.twimg.com/media/FfLMZwuVUAUCy7K.jpg) ([View Tweet](https://twitter.com/davisblalock/status/1581551166655119360))
	- In fact, you can even induce grokking through large initializations with small regularization. [4/8] 
	  
	  ![](https://pbs.twimg.com/media/FfLMaLPUoAAsKaH.jpg) ([View Tweet](https://twitter.com/davisblalock/status/1581551175052034050))
	- Their explanation for why grokking tends to happen on algorithmic datasets is that these tasks are more all-or-nothing, similar to what we see (https://t.co/qBeUGE7BHH) with exact match vs multiple choice questions for language models. [5/8] ([View Tweet](https://twitter.com/davisblalock/status/1581551176528502785))
	- Besides shedding light on grokking, this makes me way more scared of using too large an initialization, as opposed to too small an initialization. [6/8] ([View Tweet](https://twitter.com/davisblalock/status/1581551177434480640))
	- Paper: https://t.co/4ZviAHn7eo
	  
	  If you like this paper, consider RTing this (or another!) thread to publicize the authors' work, or following the authors: @ZimingLiu11… [7/8] ([View Tweet](https://twitter.com/davisblalock/status/1581551178260701185))
	- @ZimingLiu11 …@tegmark
	  
	  For more paper summaries, you might like following @mosaicml, me, or my newsletter: https://t.co/5BMBC84xY8
	  
	  As always, comments and corrections welcome! [8/8] https://t.co/jBrzSkFxRM ([View Tweet](https://twitter.com/davisblalock/status/1581551179258994688))