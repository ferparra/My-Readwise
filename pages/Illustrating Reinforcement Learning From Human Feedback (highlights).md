title:: Illustrating Reinforcement Learning From Human Feedback (highlights)
author:: [[huggingface.co]]
full-title:: "Illustrating Reinforcement Learning From Human Feedback"
category:: #articles
url:: https://huggingface.co/blog/rlhf

- Highlights first synced by [[Readwise]] [[Apr 13th, 2023]]
	- Language models have shown impressive capabilities in the past few years by generating diverse and compelling text from human input prompts. However, what makes a "good" text is inherently hard to define as it is subjective and context dependent. There are many applications such as writing stories where you want creativity, pieces of informative text which should be truthful, or code snippets that we want to be executable. ([View Highlight](https://read.readwise.io/read/01gxhpz6y3v7h0wtvkxhb4mqk4))
	- Writing a loss function to capture these attributes seems intractable and most language models are still trained with a simple next token prediction loss (e.g. cross entropy). To compensate for the shortcomings of the loss itself people define metrics that are designed to better capture human preferences such as [BLEU](https://en.wikipedia.org/wiki/BLEU) or [ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)). ([View Highlight](https://read.readwise.io/read/01gxhq0b89cr9h96yfhv1ywjc5))
	- RLHF's most recent success was its use in [ChatGPT](https://openai.com/blog/chatgpt/). ([View Highlight](https://read.readwise.io/read/01gxj1217v7x4njrdnpbqzr4ag))
	- As a starting point RLHF use a language model that has already been pretrained with the classical pretraining objectives (see this [blog post](https://huggingface.co/blog/how-to-train) for more details). OpenAI used a smaller version of GPT-3 for its first popular RLHF model, [InstructGPT](https://openai.com/blog/instruction-following/). Anthropic used transformer models from 10 million to 52 billion parameters trained for this task. DeepMind used their 280 billion parameter model [Gopher](https://arxiv.org/abs/2112.11446).
	  
	  This initial model *can* also be fine-tuned on additional text or conditions, but does not necessarily need to be. For example, OpenAI fine-tuned on human-generated text that was “preferable” and Anthropic generated their initial LM for RLHF by distilling an original LM on context clues for their “helpful, honest, and harmless” criteria. These are both sources of what I refer to as expensive, *augmented* data, but it is not a required technique to understand RLHF. ([View Highlight](https://read.readwise.io/read/01gxj13dgbc4zwrv615znc6z4z))
	- ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/pretraining.png) ([View Highlight](https://read.readwise.io/read/01gxj13gd6cq6vanf7ba8kqwhr))
	- Generating a reward model (RM, also referred to as a preference model) calibrated with human preferences is where the relatively new research in RLHF begins. The underlying goal is to get a model or system that takes in a sequence of text, and returns a scalar reward which should numerically represent the human preference. The system can be an end-to-end LM, or a modular system outputting a reward (e.g. a model ranks outputs, and the ranking is converted to reward). The output being a **scalar** **reward** is crucial for existing RL algorithms being integrated seamlessly later in the RLHF process. ([View Highlight](https://read.readwise.io/read/01gxj14bzakajk7qaedmzff0w0))
	- These LMs for reward modeling can be both another fine-tuned LM or a LM trained from scratch on the preference data. For example, Anthropic uses a specialized method of fine-tuning to initialize these models after pretraining (preference model pretraining, PMP) because they found it be more sample efficient than fine-tuning, but no one variation of reward modeling is considered the clear best choice today. ([View Highlight](https://read.readwise.io/read/01gxjc3k6a79vgzqqk27vbe7gy))