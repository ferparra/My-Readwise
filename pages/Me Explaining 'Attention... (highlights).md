title:: Me Explaining 'Attention... (highlights)
author:: [[@docmilanfar on Twitter]]
full-title:: "Me Explaining 'Attention..."
category:: #tweets
url:: https://twitter.com/docmilanfar/status/1630019692084723713

- Highlights first synced by [[Readwise]] [[Feb 28th, 2023]]
	- Me explaining 'Attention': they're filters of the form A(x) x.
	  
	  ML papers explaining ‘Attention’: 
	  https://t.co/stjVeJIiW3 ([View Tweet](https://twitter.com/docmilanfar/status/1630019692084723713))
		- **Note**: Thread
	- … where A(x) is a row-normalized affinity matrix defined with a positive semi-definite kernel 
	  
	  K(xₘ,xₙ) = exp[−d(xₘ,xₙ)]
	  
	  where d(.,.) is a metric.
	  
	  (Attention is also basically the same mechanism that as was used in nonlinear anisotropic diffusion 35 yrs ago.) 
	  
	  ![](https://pbs.twimg.com/media/Fp8wNtoaUAEhc-2.jpg) ([View Tweet](https://twitter.com/docmilanfar/status/1630074608132190208))