title:: Data Mesh - The Data Quality Control Mechanism for MLOps? // Scott Hirleman // MLOps Coffee Sessions #77 (highlights)
author:: [[MLOps.community]]
full-title:: "Data Mesh - The Data Quality Control Mechanism for MLOps? // Scott Hirleman // MLOps Coffee Sessions \#77"
category:: #podcasts

- Highlights first synced by [[Readwise]] [[Nov 19th, 2022]]
	- Data Modeling - The Second Layer
	  
	  Summary:
	  The second layer, to me, is you're shifting the responsibility of owning the data to the people who know it best the domains. You also need to go to inner operability, which i'll get into in a second. The other aspect is that for operability, you have standards, that you have the teams adhere to around the data. This is still pretty nascent to me. I haven't seen anybody get the specifics of, here's how you do these standards. Here are the standards we recommendknow and so on.
	  
	  Transcript:
	  Speaker 2
	  So i i cut you off on that second part. Then you're breaking it down. What was the second part that you was goin to say?
	  
	  Speaker 1
	  So the corollarigan is this set of data products, but like, ho great. How do you do that? Ah, the second layer, to me, is you're shifting the responsibility of owning the data to the people who know it best the domains. So the if you're not doing ndriven design, doing data meshes can be pretty difficult. The lines of business know the data best. So you want to shift the responsibility for owning that data to them so they can focus on structuring their data products in such a way that it maximizes context. You also need to go to inner operability, which i'll get you in a second, ecause otherwise you just have hquality data silos. If everybody is structuring their data however they want, and you're not thinkingow how people can use that to combine data. Yore m l engineers are going to be screaming and throwing things at me right now, right? Ah. But to enable the domains to actually handle this, you have to give them resources. Is that people that really understand data modelling that you're going to put into the domains. You're gong to take data engineers and put them into the domains. Possibly in a lot of instances, that's what people are doing. It's also that you create a self service platform, and that that self service is for a the data producers to produce data products and focus on sharing their information, not on managing impho structure, right? So they get to specifically focus on the part that matters, and not on managing kofka or, you know, whatever, streaming or spark or snowflake or any of that stuff. That stuff is managed at the platform level. The other aspect is that for operability, you have standards, that you have the teams adhere to around the data. This is still pretty nascent to me. I haven't seen anybody get the specifics of, here's how you do these standards. Here are the standards we recommendknow, i'm trying to dig into that as well, launch to podcast, a kind of dig into these very specific topics of the, you know, instead of the five ys, the five hows, just like, how do you do this? Ok, let's get specific. Let's get specific. Let's get specific. Ah, but you know, you need to think about that. Otherwise, the whole point of data mesh is to be able to combine data from a lot of different sources and be able to get a fuller picture of what's going on in the world to your business, instead of just the domains being able to share only what they're seeing, or that you have an enterprise data warehouse where it doesn't really show the full picture of what's going on with the business. And you have that product, data as a product mind set concept to evolve, right? ([TimeÂ 0:11:15](https://share.snipd.com/snip/cb5aad9f-b3bd-4da9-bdcd-116d13e9d353))