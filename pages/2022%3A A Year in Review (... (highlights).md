title:: 2022: A Year in Review (... (highlights)
author:: [[@omarsar0 on Twitter]]
full-title:: "2022: A Year in Review (..."
category:: #tweets
url:: https://twitter.com/omarsar0/status/1607080018546417665

- Highlights first synced by [[Readwise]] [[Dec 26th, 2022]]
	- 2022: A Year in Review (ML Papers Edition)
	  
	  In this thread, let's take a look at some of the top trending ML papers of 2022 â†“ 
	  
	  ![](https://pbs.twimg.com/media/Fk1-HhaWYAAN19N.png) 
	  
	  ![](https://pbs.twimg.com/media/Fk1-ahxWQAQDGJ_.jpg) 
	  
	  ![](https://pbs.twimg.com/media/Fk1-bt5X0AEixPn.jpg) 
	  
	  ![](https://pbs.twimg.com/media/Fk1-c4VWAAAzXhD.jpg) ([View Tweet](https://twitter.com/omarsar0/status/1607080018546417665))
		- **Note**: Thread
	- 1) A ConvNet for the 2020s - Liu et al.
	  
	  Vision Transformers took off this year but this work proposes ConvNeXt to reexamine the design spaces and test the limits of a pure ConvNet on several vision tasks. The ConvNets vs. Transformers debate continues.
	  
	  https://t.co/uFGY8r9C1y 
	  
	  ![](https://pbs.twimg.com/media/Fk1sWANX0AA87Rx.png) ([View Tweet](https://twitter.com/omarsar0/status/1607080020953923590))
	- 2) Language Models as Zero-Shot Planners - Huang et al.
	  
	  Studies the possibility of grounding high-level tasks to actionable steps for embodied agents. Pre-trained LLMs are used to extract knowledge to perform common-sense grounding by planning actions.
	  
	  https://t.co/IcNcZbUTtW 
	  
	  ![](https://pbs.twimg.com/media/Fk1uX5iWQAEycRP.png) ([View Tweet](https://twitter.com/omarsar0/status/1607080022984232961))
	- 3) OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework - Yang et al.
	  
	  Introduces a unified paradigm for effective multimodal pre-training that support all kinds of uni-modal and cross-modal tasks.
	  
	  https://t.co/fo1f2gaxvZ 
	  
	  ![](https://pbs.twimg.com/media/Fk1wC5-WYAciE5P.jpg) ([View Tweet](https://twitter.com/omarsar0/status/1607080025395974144))
	- 4) Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer - Yang et al.
	  
	  Proposes a new paradigm for more efficiently tuning large neural networks via zero-shot hyperparameter tuning.
	  
	  https://t.co/JtTIFDQaOV 
	  
	  ![](https://pbs.twimg.com/media/Fk1_FJuWYAEIaSW.png) ([View Tweet](https://twitter.com/omarsar0/status/1607080435519213568))
	- 5) OPT: Open Pre-trained Transformer Language Models - Zhang et al. 
	  
	  An open pre-trained transformer-based language model called OPT; follows other open-sourcing LLM efforts such as GPT-Neo; model sizes range from 125M to 175B parameters. 
	  
	  https://t.co/fx1SQaDRxt 
	  
	  ![](https://pbs.twimg.com/media/Fk1_gQGXwAEJS7E.png) ([View Tweet](https://twitter.com/omarsar0/status/1607080855029313537))
	- 6) Gato - DeepMind
	  
	  Gato is an agent built to work as a multi-modal, multi-task, multi-embodiment generalist policy; it performs all sorts of general tasks ranging from playing Atari to chatting to stacking blocks with a real robot arm.
	  
	  https://t.co/90a4CU3KIb 
	  
	  ![](https://pbs.twimg.com/media/Fk1_1MCXoAcRyxP.jpg) ([View Tweet](https://twitter.com/omarsar0/status/1607081240921874436))
	- 7) Solving Quantitative Reasoning Problems with Language Models
	  
	  Introduces Minerva, a large language model pretrained on general natural language data and further trained on technical content; evaluated on several tasks requiring quantitative reasoning.
	  
	  https://t.co/HuYpSsLLs7 
	  
	  ![](https://pbs.twimg.com/media/Fk1_9MqWQAEqSc5.png) ([View Tweet](https://twitter.com/omarsar0/status/1607081381426876416))
	- 8) No Language Left Behind (NLLB) - Meta AI
	  
	  Introduces a massive translation model (NLLB-200), capable of translating between 200 languages.
	  
	  https://t.co/y2AyRStN4W 
	  
	  ![](https://pbs.twimg.com/media/Fk2AQoQXEAEGEGg.jpg) ([View Tweet](https://twitter.com/omarsar0/status/1607081705675825154))
	- 9) Stable Diffusion - Rombach et al.
	  
	  A text-to-image model to generate detailed images conditioned on text descriptions; can be applied to other tasks such as inpainting, outpainting, and generating image-to-image translations guided by a text prompt.
	  
	  https://t.co/GbvadrQ7hC ([View Tweet](https://twitter.com/omarsar0/status/1607082562475679745))
	- 10) Whisper - OpenAI
	  
	  An open-source model called Whisper that approaches human-level robustness and accuracy in English speech recognition.
	  
	  https://t.co/U7TefUsbsy 
	  
	  ![](https://pbs.twimg.com/media/Fk2BSYTWAAEeQXn.jpg) ([View Tweet](https://twitter.com/omarsar0/status/1607082831645388800))
	- 11) Make-A-Video (Singer et al)
	  
	  Introduces a state-of-the-art text-to-video model that can generate videos from a text prompt.
	  
	  https://t.co/THFzGsjz3k https://t.co/GiTqYo1zJW ([View Tweet](https://twitter.com/omarsar0/status/1607083035056345094))
	- 12) Galactica - A large language model for science (Ross et al)
	  
	  A large language model for the science domain trained on a massive scientific corpus.
	  
	  https://t.co/eJrSdVrdpt 
	  
	  ![](https://pbs.twimg.com/media/Fk2CESWWAAcLHPu.png) ([View Tweet](https://twitter.com/omarsar0/status/1607083687367303168))
	- The list is non-exhaustive. I tried to highlight trending papers for each month of the year based on trends. 
	  
	  Feel free to share your favorite ML papers below. Happy holidays!ðŸŽ‰
	  
	  One last favor: follow me (@omarsar0) to keep track of more exciting ML papers in 2023. ([View Tweet](https://twitter.com/omarsar0/status/1607084074534846465))