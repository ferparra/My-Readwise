title:: I Was Puzzled for a Whil... (highlights)
author:: [[@yoavgo on Twitter]]
full-title:: "I Was Puzzled for a Whil..."
category:: #tweets
url:: https://twitter.com/yoavgo/status/1649720816769138689

- Highlights first synced by [[Readwise]] [[Apr 27th, 2023]]
	- I was puzzled for a while as to why we need RL for LM training, rather than just using supervised instruct tuning. I now have a convincing argument, which is also reflected in a recent talk by @johnschulman2 . I summarize it in this post:
	  
	  https://t.co/DQD1wgyjg3 ([View Tweet](https://twitter.com/yoavgo/status/1649720816769138689))
		- **Note**: Thread
	- (this is John's talk: https://t.co/1LDLB9FyEz ) ([View Tweet](https://twitter.com/yoavgo/status/1649721236841218050))
	- @unsorsodicorda @johnschulman2 hmm.. this thread seem to provide answers as to why we technically need the rl paradigm vs computing gradients directly based on the reward model. i attempt to answer a different question, which is why we need a reward model to begin with, vs just training autoregressively. ([View Tweet](https://twitter.com/yoavgo/status/1649733215051980801))