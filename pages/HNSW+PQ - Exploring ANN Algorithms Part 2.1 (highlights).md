title:: HNSW+PQ - Exploring ANN Algorithms Part 2.1 (highlights)
author:: [[weaviate.io]]
full-title:: "HNSW+PQ - Exploring ANN Algorithms Part 2.1"
category:: #articles
url:: https://weaviate.io/blog/ann-algorithms-hnsw-pq

- Highlights first synced by [[Readwise]] [[Apr 7th, 2023]]
	- Increasing the number of objects vs. storing longer dimensional vectors has an equivalent effect on the overall memory required to store the vectors. As an example consider the Gist dataset, which contains 1,000,000 vectors, each with 960 dimensions. This would mean we need roughly 500 MB for the graph but nearly ten times more memory for the vectors. On the other hand, a database such as DeepImage96 would have 96 dimensions but almost 10,000,000 vectors, meaning, that we would need around 10 GB to hold the vectors and the graph, ~5 GB for each graph. ([View Highlight](https://read.readwise.io/read/01gx9shm78c1b5t61fm7rvz9kd))
	- The main idea behind vector compression is to have a ‚Äúgood-enough‚Äù representation of the vectors (as opposed to a perfect representation) so they take up less space in memory while still allowing us to calculate the distance between them in a performant and accurate way. Compression could come from different sources. We could, for example, aim to reduce redundant data to store information more efficiently. We could also sacrifice accuracy in the data in favor of space. We aim to do the latter in this post. ([View Highlight](https://read.readwise.io/read/01gx9shf807cgcp6hd5cd17j7p))
	- ![comp1](https://weaviate.io/assets/images/image1-82834bbc63addfb73c4b96e72e4618ea.jpg) ([View Highlight](https://read.readwise.io/read/01gx9sgp3n9bw4gcsp135j2m1k))
	- ![comp2](https://weaviate.io/assets/images/image2-687fc016b43737f71a4e2c7460b6d3c8.jpg) ([View Highlight](https://read.readwise.io/read/01gx9sgqp3d3z2xg9n5btbr2nb))
	- ![ann](https://weaviate.io/assets/images/Ann-a7288e4e26411e2dc36a0602436aaac7.png) ([View Highlight](https://read.readwise.io/read/01gx9sh7htpbmwzjnx0wjzp0c9))
	- The main intuition behind Product Quantization is that it adds the concept of segments to the compression function. Basically, to compress the full vector, we will chop it up and operate on segments of it. For example, if we have a 128 dimensional vector we could chop it up into 32 segments, meaning each segment would contain 4 dimensions. Following this segmentation step we compress each segment independently. ([View Highlight](https://read.readwise.io/read/01gx9shapyn6496q64zx1ek1ha))
	- ![pq](https://weaviate.io/assets/images/image3-4c327e01fd7dede8a179ad3864c2ba84.jpg) ([View Highlight](https://read.readwise.io/read/01gx9sgvk1zq1q1jh71fythcw4))
	- The main idea behind running these experiments is to explore how PQ compression would affect our current indexing algorithms. The experiments consist of fitting the Product Quantizer on some datasets and then calculating the recall by applying brute force search on the compressed vectors. This will give us two important results, the time it would take to fit the data with KMeans clustering and compress the vectors (which will be needed at some step by the indexing algorithm) and the distortion introduced by reconstructing the compressed vectors to calculate the distance. This distortion is measured in terms of a drop in recall. ([View Highlight](https://read.readwise.io/read/01gx9sh1725g0n04jmym0bqxzs))
	- Conclusions[‚Äã](https://weaviate.io/blog/ann-algorithms-hnsw-pq#conclusions)
	  
	  In this post we explore the journey and details behind the HNSW+PQ feature released in Weaviate v1.18. Though we still have a ways to go on this journey, as noted earlier, but what we've accomplished thus far can add loads of value to Weaviate users. This solution already allows for significant savings in terms of memory. When compressing vectors ranging from 100 to 1000 dimensions you can save half to three fourths of the memory you would normally need to run HNSW. This saving comes with a little cost in recall or latency. The indexing time is also larger but to address this we've cooked up a second encoder, developed specially by Weaviate, based on the distribution of the data that will reduce the indexing time significantly. Keep your eyes peeled for more details on this soon on [our blog](https://weaviate.io/blog)! We will share our insights as we go. üòÄ ([View Highlight](https://read.readwise.io/read/01gx9sj6cbg4se2wd2grjqy5rc))