title:: 306 – Oriol Vinyals —  Deep Learning and Artificial General Intelligence (highlights)
author:: [[Lex Fridman Podcast]]
full-title:: "\#306 – Oriol Vinyals —  Deep Learning and Artificial General Intelligence"
category:: #podcasts
url:: https://share.snipd.com/episode/1f05933c-eb8e-41c4-8945-039f5185afe3

- Highlights first synced by [[Readwise]] [[Nov 19th, 2022]]
	- What is Attention?
	  
	  Summary:
	  In a people that study cognition o human attention, i think thereis giant wars what attention means. What these very simple looks at what attention is in mour network from the days of attention is all you need. But bro, do you think there's a general principle that's is really powerful here? Yes, o, a distinction between transformers and els tms, which were what came before.
	  
	  Transcript:
	  Speaker 1
	  That is a surprise that keeps recurring into other projects.
	  
	  Speaker 2
	  Try to on a philosophical technical level introspect what is the magic of attention? What is attention? It's attention in people that study cognition, so human attention. I think there's giant wars over what attention means, how it works in the human mind. So what are the very simple looks at what attention is in neural network from the days of attention is all you need. But do you think there's a general principle that's really powerful here?
	  
	  Speaker 1
	  Yeah, so a distinction between transformers and LSTMs, which were what came before. And there was a transitional period where you could use both. In fact, when we talked about AlphaStack, we used transformers and LSTMs. So it was still the beginning of transformers, they were very powerful, but LSTMs were also very powerful sequence models. So the power of the transformer is that it ([Time 1:20:16](https://share.snipd.com/snip/950ed339-23c7-4d02-8ace-45624131b29c))