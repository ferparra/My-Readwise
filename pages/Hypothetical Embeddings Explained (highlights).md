title:: Hypothetical Embeddings Explained (highlights)
author:: [[summarity.com]]
full-title:: "Hypothetical Embeddings Explained"
category:: #articles
url:: https://summarity.com/hyde

- Highlights first synced by [[Readwise]] [[May 15th, 2023]]
	- In LLM-only systems, such as search engines powered by [RLHF](https://huggingface.co/blog/rlhf)-tuned models, thereâ€™s a risk of the core model confidently stating wrong facts or making assertions that go beyond or even contradict the available ground truths (e.g. a search index). This problem is more commonly known as [hallucination](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)). ([View Highlight](https://read.readwise.io/read/01h06pc13jf6qhgwkk22cxjex0))
	- There are [several ways](https://cobusgreyling.medium.com/preventing-llm-hallucination-with-contextual-prompt-engineering-an-example-from-openai-7e7d58736162) to engineer prompts to minimize this effect, and combined with [instruction fine tuning](https://arxiv.org/abs/2210.11416) are generally successful in limiting hallucinations. In systems where fact retrieval is the primary use case though, this problem can be almost entirely eliminated. ([View Highlight](https://read.readwise.io/read/01h06pc7fwrxjmp9nmr90rv8f6))
	- HyDE-powered search relies on embeddings, dense vector representations of (in this case) textual data. [OpenAI embeddings](https://platform.openai.com/docs/guides/embeddings) for example, can effectively encode abstract qualities of short and long-form text (up to ~8,000 tokens per embedding) in ~1600-dimensional vectors. These vectors can then be stored in vector-similarity (VSS) indexes (e.g. [HNSW/RediSearch](https://redis.io/docs/stack/search/reference/vectors/#querying-vector-fields)) that facilitate efficient VSS or [hybrid VSS and full-text queries](https://redis.io/docs/stack/search/reference/vectors/#hybrid-knn-queries). ([View Highlight](https://read.readwise.io/read/01h06pbqkbn4w3rmjw2aa7xh80))
	- Instead of embedding the user input directly, a sufficiently trained LLM can be used to generated *hypothetical* answers, which are in turn embedded and used for retrieval. The LLM will likely hallucinate details for its answers, but since these answers are not relayed to the user as fact, these errors are inconsequential. The embedding is much more sensitive to the structure and language used in the answers, rather than its correctness. ([View Highlight](https://read.readwise.io/read/01h06pd7bw7trrz0kmx5dsdwx3))