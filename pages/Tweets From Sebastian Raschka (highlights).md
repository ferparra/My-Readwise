title:: Tweets From Sebastian Raschka (highlights)
author:: [[@rasbt on Twitter]]
full-title:: "Tweets From Sebastian Raschka"
category:: #tweets
url:: https://twitter.com/rasbt

- Highlights first synced by [[Readwise]] [[Apr 13th, 2023]]
	- Since self-attention is now everywhere, it's important to understand how it works.
	  And there is no better and more fun way than coding it from scratch!
	  My new article on "Understanding the Self-Attention Mechanism of Large Language Models From Scratch"
	  ðŸ‘‰ https://t.co/zFsWH4orpa ([View Tweet](https://twitter.com/rasbt/status/1623705367333986305))
- New highlights added [[Apr 27th, 2023]] at 12:21 PM
	- Thanks to parameter-efficient finetuning techniques, you can finetune a 7B LLM on a single GPU in 1-2 h using techniques like low-rank adaptation (LoRA).
	  
	  Just wrote a new article explaining how LoRA works & how to finetune a pretrained LLM like LLaMA: 
	  https://t.co/zxi4gNcJdX ([View Tweet](https://twitter.com/rasbt/status/1651226178353614854))