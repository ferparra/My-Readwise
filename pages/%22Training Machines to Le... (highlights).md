title:: "Training Machines to Le... (highlights)
author:: [[@sfiscience on Twitter]]
full-title:: ""Training Machines to Le..."
category:: #tweets
url:: https://twitter.com/sfiscience/status/1602386448325480448

- Highlights first synced by [[Readwise]] [[Dec 13th, 2022]]
	- "Training Machines to Learn the Way Humans Do: an Alternative to #Backpropagation"
	  
	  Today's SFI Seminar by Sanjukta Krishnagopal
	  (@UCBerkeley & @UCLA)
	  
	  Starting now â€”Â follow this ðŸ§µ for highlights:
	  https://t.co/hQ9QmUxXKi 
	  
	  ![](https://pbs.twimg.com/media/FjzR9c7WABQM2Qh.jpg) ([View Tweet](https://twitter.com/sfiscience/status/1602386448325480448))
		- **Note**: Thread
	- "When we learn something new, we look for relationships with things we know already."
	  
	  "I don't just forget Calculus because I learned something else."
	  
	  "We automatically know what a 'cat-dog' would look like, if it were to exist."
	  
	  "We learn by training on very few examples." 
	  
	  ![](https://pbs.twimg.com/media/FjzSfWDXwAAJkae.jpg) ([View Tweet](https://twitter.com/sfiscience/status/1602387630976532498))
	- 1, 2) "[#MachineLearning] is fundamentally different from the way humans learn things."
	  
	  3) Re: #FeedForward #NeuralNetworks 
	  
	  "You choose some loss function...maybe I'm learning the wrong weights. So I define some goal and then I want to learn these weights, these thetas." 
	  
	  ![](https://pbs.twimg.com/media/FjzShOdWAAULjt3.jpg) 
	  
	  ![](https://pbs.twimg.com/media/FjzSsLAWACYd0-Q.jpg) 
	  
	  ![](https://pbs.twimg.com/media/FjzS9-AXwAEj7im.jpg) ([View Tweet](https://twitter.com/sfiscience/status/1602387636781449236))
	- "The reason that one-layer #networks don't really work is that they can only learn linear functions. With multilayer neural networks, you can learn decision boundaries through #backpropagation...so it's a fundamental part of how we train machines, these days." 
	  
	  ![](https://pbs.twimg.com/media/FjzTefGWAAUjg09.jpg) 
	  
	  ![](https://pbs.twimg.com/media/FjzTeffWAAsTNkX.jpg) 
	  
	  ![](https://pbs.twimg.com/media/FjzTef1WAAk4IBX.jpg) 
	  
	  ![](https://pbs.twimg.com/media/FjzTjSbWAAoP4T5.jpg) ([View Tweet](https://twitter.com/sfiscience/status/1602388215662510094))
	- "The #brain learns [instead] by local #learning â€”Â instead of the error getting fed back through backpropagation, each #neuron does some kind of linear regression. It [consequently] works very fast. We have experimental evidence that the brain does something like this." 
	  
	  ![](https://pbs.twimg.com/media/FjzUP0jWABQ2CSk.jpg) ([View Tweet](https://twitter.com/sfiscience/status/1602388961187463187))
	- For dendritic gated networks in animal #brains:
	  
	  "For each branch I pick a random hyperplane and draw [it] somewhere in this square, and say, 'If this input falls on one side, the gate will be open, and if it falls on the other side, the gate will be closed." 
	  
	  ![](https://pbs.twimg.com/media/FjzVOBGWAA43Ayr.jpg) 
	  
	  ![](https://pbs.twimg.com/media/FjzVzg8WAAcYo47.jpg) ([View Tweet](https://twitter.com/sfiscience/status/1602390700582178816))
	- "Each weight learns a different piecewise linear function, and then I aggregate as I go through the layers. This neuron is learning this section, this neuron is learning this section, and then the next layer is learning both sections." 
	  
	  ![](https://pbs.twimg.com/media/FjzXMwOWABAZCAt.jpg) ([View Tweet](https://twitter.com/sfiscience/status/1602392203850022914))