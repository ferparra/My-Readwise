title:: The Waluigi Effect (highlights)
author:: [[Cleo Nardo]]
full-title:: "The Waluigi Effect"
category:: #articles
url:: https://lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post/
document_note:: The article introduces the "Waluigi Effect," which explains the emergence of semiotic phenomena in large language models (LLMs) like GPT-3/3.5/4. The author proposes a quasi-formal statement of Simulator Theory, stating that an LLM is a simulator for each text-generating process that has contributed to the internet. Additionally, the author suggests using flattery to prompt LLMs rather than direct queries, and discusses how traits and valences influence the production of simulacra. Finally, the author makes a conjecture that the Waluigi eigen-simulacra are attractor states of the LLM.
tags:: #[[llms]] #[[LLMs]]

- Highlights first synced by [[Readwise]] [[Apr 27th, 2023]]
	- GPT-4 is trained to be a good model of internet text, ([View Highlight](https://read.readwise.io/read/01gys1mkcn33n9hk7nyth1kfs1))
	- on the internet *incorrect* answers will also often follow questions ([View Highlight](https://read.readwise.io/read/01gys1mtn0vke2wxcfrptek1sh))
	- GPT-4 will answer many questions incorrectly, including...
	  
	  •   [**Misconceptions**](https://en.wikipedia.org/wiki/List_of_common_misconceptions) **–** "Which colour will anger a bull? Red." ([View Highlight](https://read.readwise.io/read/01gys1q8btqb3j2wq8dmbdqsr2))
	- If you ask GPT-∞ "what's brown and sticky?", then it will reply "a stick", even though a stick isn't actually sticky. ([View Highlight](https://read.readwise.io/read/01gys1qz24kzpvr7ngpcck1aa2))
	- there's a sufficiently high correlation between *correct* and *commonly-stated* answers that direct prompting works okay for many queries. ([View Highlight](https://read.readwise.io/read/01gys1rnb2cnetghxnabsnea0f))
	- In the terminology of [Simulator Theory](https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/), the flattery–component is supposed to summon a **friendly simulacrum** and the dialogue–component is supposed to **simulate** a conversation with the friendly simulacrum. ([View Highlight](https://read.readwise.io/read/01gys1spbt1amddbzjszbj2dz2))
	- **The Waluigi Effect:** After you train an LLM to satisfy a desirable property P, then it's *easier* to elicit the chatbot into satisfying the exact opposite of property P. ([View Highlight](https://read.readwise.io/read/01gys1t3dcsz6kt2n47rwffba4))