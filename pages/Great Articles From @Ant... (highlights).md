title:: Great Articles From @Ant... (highlights)
author:: [[@DrJimFan on Twitter]]
full-title:: "Great Articles From @Ant..."
category:: #tweets
url:: https://twitter.com/DrJimFan/status/1611058679385837569

- Highlights first synced by [[Readwise]] [[Jan 6th, 2023]]
	- Great articles from @AnthropicAI that reveal the intriguing process of how neural networks internalize training data. Key takeaways: üßµ
		- **Note**: Thread
	- Overfitting happens when NNs store *data points* in superposition.
	- Does more data always reduce overfitting? No. ‚ÄúData double descent‚Äù is a surprising phenomenon where test loss gets *worse* before it gets better as we scale the dataset.
	- Lots of great experiments and explanations in the following readings: 
	  * Introduces superposition: https://t.co/wP6Ar83Zbz
	  * Data double descent: https://t.co/Z1k8UA9nRg
	  
	  END/üßµ ([View Tweet](https://twitter.com/DrJimFan/status/1611058691217948672))