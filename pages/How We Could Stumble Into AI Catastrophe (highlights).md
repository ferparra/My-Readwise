title:: How We Could Stumble Into AI Catastrophe (highlights)
author:: [[Cold Takes Audio]]
full-title:: "How We Could Stumble Into AI Catastrophe"
category:: #podcasts
url:: https://share.snipd.com/episode/a37789da-7346-4b13-84e6-9e94854675eb

- Highlights first synced by [[Readwise]] [[Jan 17th, 2023]]
	- How to Do Better AI Projects
	  
	  Key takeaways:
	  (* Nobody wants to unilaterally slow themselves down in order to be cautious., * The situation might be improved if we could develop a set of standards that AI projects need to meet and enforce the standards evenly across a broad set of companies or even internationally., * This is not just about buying time, it's about creating incentives for companies to prioritize safety., ..., * For a simple example, imagine an AI company in a dominant market position months ahead of all the competition in some relevant sense, like its AI systems are more capable such that it would take the competition months to catch up., * Such a company could put huge amounts of resources, including its money, top people and its advanced AI systems themselves, into AI safety research.)
	  
	  Transcript:
	  Speaker 1
	  Nobody wants to unilaterally slow themselves down in order to be cautious. The situation might be improved if we could develop a set of standards that AI projects need to meet and enforce the standards evenly across a broad set of companies or even internationally. This is not just about buying time, it's about creating incentives for companies to prioritize safety. An analogy might be something like the Clean Air Act or fuel economy standards. We might not expect individual companies to voluntarily slow down product releases while they work on reducing pollution, but once required, reducing pollution becomes part of what they need to do to be profitable. Now standards could be used for things other than alignment risk as well. AI projects might be required to take strong security measures, test models before release to understand what they can be used for, restrict access to models that can be used for dangerous things, so it could make a lot of these things better. And I talk about it more in a previous piece and probably in an expandable box. Another way we could do better, successful, careful AI projects. I think a single AI company or other AI project could enormously improve the situation, if it could be both successful and careful. For a simple example, imagine an AI company in a dominant market position months ahead of all the competition in some relevant sense, like its AI systems are more capable such that it would take the competition months to catch up. Such a company could put huge amounts of resources, including its money, top people and its advanced AI systems themselves, into AI safety research, hoping to find safety measures that can be published for everyone else to use. It can also take a variety of other measures laid out in a previous piece and in probably an expanding box. A final way we can do better, strong security. A key threat in the above scenarios is that an in cautious actor could steal an AI system from a company or project that would otherwise be careful. By understanding is that based on the current state of security, it could be extremely hard for an AI project to be safe against this outcome. But this could change if there is enough effort to work out the problem of how to develop a large-scale powerful AI system that is very hard to steal. In future pieces, I'll get more concrete about what specific people and organizations can do today to improve the odds of factors like these going well and overall to raise the odds of a good outcome. ([TimeÂ 0:26:40](https://share.snipd.com/snip/8ef45ddc-fc49-42e3-83e3-9ef08518b59c))