title:: Hereâ€™s a Hot Take: Fine-... (highlights)
author:: [[@jerryjliu0 on Twitter]]
full-title:: "Hereâ€™s a Hot Take: Fine-..."
category:: #tweets
url:: https://twitter.com/jerryjliu0/status/1593283660702048256

- Highlights first synced by [[Readwise]] [[Feb 4th, 2023]]
	- Hereâ€™s a hot take: Fine-tuning your own LLMâ€™s on top of GPT-3/4/N should not be the only ingredient to build a differentiable data moat! 
	  The future will be focused on exploiting pretrained LLMâ€™s for reasoning rather than knowledge. Hereâ€™s some reasons why ðŸ§µÂ : ([View Tweet](https://twitter.com/jerryjliu0/status/1593283660702048256))
		- **Note**: Thread
	- Foundation models are getting a lot better very quickly. GPT-4 is apparently a massive leap (@sama, @Scobleizer) from GPT-3 in terms of performance.
	  
	  
	  Which begs the question: What marginal benefit do your custom fine-tuned models really add on top of this? ([View Tweet](https://twitter.com/jerryjliu0/status/1593283662484643841))
	- One reason to fine-tune your model could be to feed in custom knowledge that wasnâ€™t available to GPT at the time of training. But the downsides are 
	  1) finetuning is super expensive
	  2) thereâ€™s actually a ton of other ways to interact with external knowledge sources! ([View Tweet](https://twitter.com/jerryjliu0/status/1593283664355344384))
	- There's lots of great research/tools exploring GPTâ€™s ability to reason and interact with external data: chain-of-thought (@goodside, @ShunyuYao12, @_akhaliq, @OfirPress, @_jasonwei, @dmdohan etc.), tools (@LangChainAI, @dust4ai, a quick plug for GPT Index by yours truly..) ([View Tweet](https://twitter.com/jerryjliu0/status/1593283665970143232))
	- Thereâ€™s more: thereâ€™s text to SQL to db, text to Wikipedia, text to Excel actions (@shubroski) all powered by GPT; @AdeptAILabsâ€™s transformers are around interactions with our existing tools; and so much more I havenâ€™t seen yetâ€¦ ([View Tweet](https://twitter.com/jerryjliu0/status/1593283667324850176))
	- All of these provide novel ways for GPT to process your custom data and deliver value beyond just finetuning! 
	  
	  Instead of only finetuning on a custom knowledge source, ask yourself if you can use GPT as a reasoning/interaction processor on that data instead. ([View Tweet](https://twitter.com/jerryjliu0/status/1593283668755169281))
	- To me, the best part of GPT is in its ability to REASON, not memorize more world knowledge. echoes @alexandr_wang  https://t.co/M6O3cYV6Ts. 
	  
	  
	  
	  Software was built on top of a CPU and memory/hard drive; differentiated apps can be built on GPT as a reasoning engine + data. ([View Tweet](https://twitter.com/jerryjliu0/status/1593283670126694400))
	- To be clear, I think the data moat can still exist as @AlexTamkin put it https://t.co/yvLFSYPtU7. But companies building on top of GPT should carefully think about how to best develop an LLM pipeline to take advantage of this data moat - finetuning isnâ€™t the only answer! ([View Tweet](https://twitter.com/jerryjliu0/status/1593283671775068162))