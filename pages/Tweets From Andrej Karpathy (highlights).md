title:: Tweets From Andrej Karpathy (highlights)
author:: [[@karpathy on Twitter]]
full-title:: "Tweets From Andrej Karpathy"
category:: #tweets
url:: https://twitter.com/karpathy

- Highlights first synced by [[Readwise]] [[Nov 19th, 2022]]
	- How to become expert at thing:
	  1 iteratively take on concrete projects and accomplish them depth wise, learning ‚Äúon demand‚Äù (ie don‚Äôt learn bottom up breadth wise)
	  2 teach/summarize everything you learn in your own words
	  3 only compare yourself to younger you, never to others ([View Tweet](https://twitter.com/search?q=How%20to%20become%20expert%20at%20thing%3A%201%20iteratively%20take%20on%20concrete%20projects%20and%20accomplish%20them%20depth%20wise%2C%20learning%20%E2%80%9Con%20demand%E2%80%9D%20%28ie%20don%E2%80%99t%20learn%20bottom%20up%20breadth%20wise%29%202%20teach/summarize%20everything%20you%20learn%20in%20your%20own%20words%203%20only%20compare%20your%20%28from%3A%40karpathy%29))
	- A friend yesterday mentioned that semiconductor tech is probably the deepest node in our civilization's explored tech tree. This actually sounds right, but is also a fun concept, any other candidates? ([View Tweet](https://twitter.com/karpathy/status/1429478697946402816))
- New highlights added [[Dec 6th, 2022]] at 7:05 PM
	- Potentially nitpicky but competitive advantage in AI goes not so much to those with data but those with a data engine: iterated data aquisition, re-training, evaluation, deployment, telemetry. And whoever can spin it fastest. Slide from Tesla to ~illustrate but concept is general 
	  
	  ![](https://pbs.twimg.com/media/FjPQ8HqVQAAXRaw.jpg) ([View Tweet](https://twitter.com/karpathy/status/1599852921541128194))
- New highlights added [[Apr 7th, 2023]] at 5:55 PM
	- The analogy between GPTs of today to the CPUs of early days of computing are interesting. GPT is a funny kind of programmable text computer. Have to think through it more ü§î but e.g.:
- New highlights added [[Apr 13th, 2023]] at 9:25 AM
	- This is a baby GPT with two tokens 0/1 and context length of 3, viewing it as a finite state markov chain. It was trained on the sequence "111101111011110" for 50 iterations. The parameters and the architecture of the Transformer modifies the probabilities on the arrows.
	  
	  E.g. we‚Ä¶ 
	  
	  ![](https://pbs.twimg.com/media/FtSa2CGacAAVphm.jpg) ([View Tweet](https://twitter.com/karpathy/status/1645115622517542913))
- New highlights added [[Apr 15th, 2023]] at 11:58 AM
	- Random note on k-Nearest Neighbor lookups on embeddings: in my experience much better results can be obtained by training SVMs instead. Not too widely known.
	  
	  Short example:
	  https://t.co/RXO9xiOmAB
	  
	  Works because SVM ranking considers the unique aspects of your query w.r.t. data. ([View Tweet](https://twitter.com/karpathy/status/1647025230546886658))