title:: The Scaling Hypothesis (highlights)
author:: [[gwern.net]]
full-title:: "The Scaling Hypothesis"
category:: #articles
url:: https://www.gwern.net/Scaling-hypothesis

- Highlights first synced by [[Readwise]] [[Nov 19th, 2022]]
	- Models like [[GPT-3]] suggest that large unsupervised models will be vital components of future DL systems, as they can be ‘plugged into’ systems to immediately provide understanding of the world, humans, natural language, and reasoning.
	  id:: 5173212d-72a6-4565-b642-36d94cbd8091
	- The strong scaling hypothesis is that, once we find a scalable architecture like self-attention or convolutions, which like the brain can be applied fairly uniformly (eg. “The Brain as a Universal Learning Machine” or Hawkins), we can simply train ever larger NNs and ever more sophisticated behavior will emerge naturally as the easiest way to optimize for all the tasks & data.
	- AI critics often say that the long tail of scenarios for tasks like self-driving cars or natural language can only be solved by true generalization & reasoning