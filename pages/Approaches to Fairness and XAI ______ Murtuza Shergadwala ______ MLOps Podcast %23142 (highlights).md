title:: Approaches to Fairness and XAI // Murtuza Shergadwala // MLOps Podcast #142 (highlights)
author:: [[MLOps.community]]
full-title:: "Approaches to Fairness and XAI // Murtuza Shergadwala // MLOps Podcast \#142"
category:: #podcasts
url:: https://share.snipd.com/episode/75cd6216-874a-4781-85a8-bd55d62c627a

- Highlights first synced by [[Readwise]] [[Jan 29th, 2023]]
	- The Evolution of Explainability in Machine Learning
	  
	  Key takeaways:
	  (* Catering to the needs of both business and utility is important when designing dashboards and other nitty gritty details for machine learning applications., * The explainability of machine learning models is important, and humans should be able to understand them even if they have biases., * Training people who are responsible for looking at these interfaces and understanding the limitations of machine learning models is crucial.)
	  
	  Transcript:
	  Speaker 1
	  What what we have realized is that the majority of the people that the clients that we talk to, it's funny that the decision makers are at the sea level, but then the people who are actually aiding the decision are the data scientists and who ultimately talk about whether this thing is useful or not. So catering to the needs of both is important, both from a business standpoint, as well as a utility standpoint, that dashboards and all those nitty gritty details are going to be seen by data scientists or MLAs. In fact, we go to the level of separating our data scientists and MLAs because MLAs might be incentivized in a different way as compared to data scientists for their jobs, where Emily is scared about productionizing the data, they care about the model performance, and so on, whereas data scientists might care about the quality of the features, bringing in their domain expertise. They want to be able to understand if any of the features are drifting rather than performance. And so catering to both of these demographics becomes very important.
	  
	  Speaker 2
	  I'm Emmanuel Mason, machine learning engineer at Stripe and author of Building Machine Learning Powered Applications. And if you don't want your machine learning models to explode, well, you should subscribe to this podcast. One thing that I think about is something that I think you spoke to a little while back, and it's how explainability and just explainable AI as a movement and as something that is important in this field is very much changing as the field changes. And it is evolving with the field and considering how fast the field is evolving, whether that be on the modeling side or on the infrastructure side, the explainability is trying to keep up with that evolution. And so as you have your hat on of thinking about humans and computers interacting together, how do you plan for that type of thing? Like that happening basically.
	  
	  Speaker 1
	  My perspective, since I'm a human centered design guy, what I have noticed is that these explanations or these post hoc explanations that we have for explainability or these attribution based approaches, they are really model centric. So they're trying to explain the model, but at least they make an attempt to. Whereas we are trying to present it in a very human centric way. And that's where I think the friction is coming in, where yes, ultimately on a higher level, we are trying to explain the model. So we want it to be the human can build trust and understand it. But the techniques that are there, they are faithful to the model. And the way we are trying to present it is that humans are able to put in their own cognitive biases and try to really bridge the gap between what they see versus what they are thinking of. And the example for this is like, for example, if you consider image explanations, and you have a model that is doing a classification of something like it's a bird or not a bird. And we are trying to understand why does the model look at a particular image and claim it to be a bird. And we as humans have certain features in our head that it should look at the beak or it should look at the body and it should look at the wing and so on. And hopefully it's not looking at the sky or the clouds and labeling it to be a bird. But when you look at these explanation post-doc explanation approaches that are trying to show you where the model in general is trying to focus on, it is just telling you that like it's telling you that these are the regions where the model is focusing on. And we are the ones who are trying to impose our biases that, okay, if it is looking at this region, that means it has to be looking at the beak or it has to be looking at the eye or it has to be looking at the sky. That's something that we are really adding on top of it. And I think that's where the big gap lies with explainability. That is training, training the people who are actually looking at these interfaces and letting them know the limitation and assumptions of these approaches is very crucial. And we can't sell it as a tool that is for anyone and everyone. Like it's not like some person who has not designed the system or a data scientist who does not understand post-doc explanations and their limitations can look at this image and look at the heat map, for example, and we convince that the model is paying attention to these hottest regions, which means that okay, it works. That's that there's a big gap there. And I think it needs to be fitted.
	  
	  Speaker 2
	  The thing that I'm thinking about is you mentioned how you can't sell it to just anyone. It has to be a someone who is interested in this, but also has a almost like a vested interest because it is important for their job or whatever they're doing. ([TimeÂ 0:15:09](https://share.snipd.com/snip/b14983ea-7fe5-4f30-9748-400cab1ee2ca))