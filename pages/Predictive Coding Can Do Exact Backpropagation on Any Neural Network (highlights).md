title:: Predictive Coding Can Do Exact Backpropagation on Any Neural Network (highlights)
author:: [[arxiv.org]]
full-title:: "Predictive Coding Can Do Exact Backpropagation on Any Neural Network"
category:: #articles
url:: https://arxiv.org/abs/2103.04689

- Highlights first synced by [[Readwise]] [[Nov 19th, 2022]]
	- Intersecting neuroscience and deep learning has brought benefits and
	  developments to both fields for several decades, which help to both understand
	  how learning works in the brain, and to achieve the state-of-the-art
	  performances in different AI benchmarks.
		- **Tags**: #[[favorite]]
	- Backpropagation (BP) is the most
	  widely adopted method for the training of artificial neural networks, which,
	  however, is often criticized for its biological implausibility (e.g., lack of
	  local update rules for the parameters).
	- Recent works prove that IL can approximate BP up to a
	  certain margin on multilayer perceptrons (MLPs), and asymptotically on any
	  other complex model, and that zero-divergence inference learning (Z-IL), a
	  variant of IL, is able to exactly implement BP on MLPs.