title:: "A/B Testing 1.0‚Äù Was Bo... (highlights)
author:: [[@EleanorKalina on Twitter]]
full-title:: ""A/B Testing 1.0‚Äù Was Bo..."
category:: #tweets
url:: https://twitter.com/EleanorKalina/status/1409633989384540160

- Highlights first synced by [[Readwise]] [[Nov 19th, 2022]]
	- "A/B testing 1.0‚Äù was born as a sequential, iterative process done upper funnel, driving quick learnings based on high quantity signals, for marketers. 
	  
	  In a B2B context, this raises some significant obstacles to realizing ROI.
	  
	  A breakdown of challenges & alternatives.
	  
	   üëá 
	  
	  ![](https://pbs.twimg.com/media/E5AGn2CUYAELaSq.png) ([View Tweet](https://twitter.com/EleanorKalina/status/1409633966810750983))
		- **Note**: Thread
	- The traditional process of website experimentation involves gathering data, basing a hypothesis around it ("we need to move the pricing higher on the page for visitors to convert"), building experiments, evaluating results. ([View Tweet](https://twitter.com/EleanorKalina/status/1409633968400396290))
	- Typically, within these experiments, traffic is randomly allocated to one or more variations, as well as the control experience.  
	  
	  Tests conclude when there is statistically significant change in an onsite conversion goal or the test is deemed inconclusive (happens frequently). ([View Tweet](https://twitter.com/EleanorKalina/status/1409633969738424321))
	- However, this is not enough for B2B.  
	  
	  Seeking statistically significant outcomes on onsite metrics often means that traditional website experimentation becomes a traffic-based exercise, not a value-based one. ([View Tweet](https://twitter.com/EleanorKalina/status/1409633971122577417))
	- While it may still be good enough for B2C sites (e.g. ecommerce, travel), where traffic and revenue are highly correlated, it is void in many B2B scenarios. Why? ([View Tweet](https://twitter.com/EleanorKalina/status/1409633972439506954))
	- Reason #1 - It's not 'quick and dirty'.
	  
	  Due to the time it will take to see results - the longer it takes, the more money needs to be poured into the test until results are seen, the more a 'simple' test becomes expensive and heavy. ([View Tweet](https://twitter.com/EleanorKalina/status/1409633973777567753))
	- This inherently linear nature of testing, combined with the time it takes to produce statistically significant results and the low experiment win rate, makes actually getting meaningful results from a B2B testing program a long and super expensive process. ([View Tweet](https://twitter.com/EleanorKalina/status/1409633975249768450))
	- Reason #2 - it doesn't accommodate down-funnel outcomes.
	  
	  Optimizing onsite conversions means making too many assumptions about what happens down funnel. In complex funnels with numerous variables, 20% more leads via an LP CTA does not mean anything about actual revenue or deals. ([View Tweet](https://twitter.com/EleanorKalina/status/1409633976583561218))
	- Reason #3 - it is resource-intensive to do right.
	  
	  Since the typical b2b funnel contains several more steps, any change in the softer, higher parts of the funnel will require the rest of the funnel to adapt. ([View Tweet](https://twitter.com/EleanorKalina/status/1409633978009620493))
	- Reason #4 - long sales cycle require deeper cohort based analysis. 
	  
	  Sales cycle can take months and more unexpected exogenous factors come into play. Additionally, the farther away conversion is from lead gen, the better it is to analyze by cohorts, which can overshadow testing. ([View Tweet](https://twitter.com/EleanorKalina/status/1409633979385319430))
	- Reason #5 - Traffic Complexity and Visitor Context
	  
	  Customer journey is full of different motivations, expectations and approaches.  SMBs expect a free trial and low priced plan.  Enterprise customers often want ongoing support and expect to speak to sales. ([View Tweet](https://twitter.com/EleanorKalina/status/1409633980878520324))
		- **Tags**: #[[favorite]]
	- Reason #6 - (mostly for enterprise) - the need to market and deliver experiences to both accounts and individuals.  
	  
	  With over 6 decision makers involved in an enterprise deal, you must be able to speak to both the motivations of the persona/role as well as their account. ([View Tweet](https://twitter.com/EleanorKalina/status/1409633982287728650))
	- Is there a better way to test or sandbox B2B conversions? yes.  
	  
	  Rather than testing populations in aggregate, it's probably preferable to test predictions on a segmented, or even 1:1 basis based on all of the available context and historical outcomes (similar to 'lookalikes'). ([View Tweet](https://twitter.com/EleanorKalina/status/1409633983684517899))
	- Performance marketers know this from outbound efforts, but the logic behind testing predictions based on past behavior of converted customers, reduces much of the noise. ([View Tweet](https://twitter.com/EleanorKalina/status/1409633985043472392))
	- The combination of:
	  1. An already-segmented audience
	  2. Specific test to maximize impact
	  
	  Will end up being more impactful and less costly - both onsite and for down funnel success. ([View Tweet](https://twitter.com/EleanorKalina/status/1409633986477916164))
	- For example, we can personalise a page based on the persona within the company and the type of company (CMO at an SMB vs a designer in an enterprise) - no testing, but rather predicting based on past segment data. Only then the testing begins, resulting in more garnular insights. ([View Tweet](https://twitter.com/EleanorKalina/status/1409633988000444416))
	- TL;DR - 
	  
	  B2B testing is challenging due to: