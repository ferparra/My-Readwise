title:: As State-of-the-Art AI A... (highlights)
author:: [[@davidad on Twitter]]
full-title:: "As State-of-the-Art AI A..."
category:: #tweets
url:: https://twitter.com/davidad/status/1609208381511979008

- Highlights first synced by [[Readwise]] [[Jan 1st, 2023]]
	- As state-of-the-art AI architectures have become less and less purely-“stack more layers” (look at Stable Diffusion!), it might be interesting to take another (squinty) look at some of Minsky’s Society of Mind ideas if you haven’t flipped through the original any time recently. 
	  
	  ![](https://pbs.twimg.com/media/FlUOfUGXoAEHLEx.jpg) 
	  
	  ![](https://pbs.twimg.com/media/FlUOfT8WAAEZSa_.jpg) 
	  
	  ![](https://pbs.twimg.com/media/FlUOfUKWAAQd2Ad.jpg) 
	  
	  ![](https://pbs.twimg.com/media/FlUOfULXwAIR4bB.jpg) ([View Tweet](https://twitter.com/davidad/status/1609208381511979008))
		- **Note**: Thread
	- ![](https://pbs.twimg.com/media/FlUPiMtWYAAikvx.jpg) 
	  
	  ![](https://pbs.twimg.com/media/FlUPiNyWYAU7EkW.jpg) ([View Tweet](https://twitter.com/davidad/status/1609209528024793089))
	- @occamsbulldog I would even argue that, in hindsight, self-attention and cross-attention “layers” represented a shift from something that looks like a feedforward convolutional perceptron which just happens to be very deeply stacked, to something that looks more like recurrent Minsky K-lines. ([View Tweet](https://twitter.com/davidad/status/1609271578600312836))