title:: The Dream Machine: JCR Licklider and the Revolution That Made Computing Personal (highlights)
author:: [[Blas]]
full-title:: "The Dream Machine: JCR Licklider and the Revolution That Made Computing Personal"
category:: #articles
url:: https://blas.com/the-dream-machine/
document_note:: JCR Licklider was a visionary who saw the potential for computers to become humane and individual, to democratize access to information, and to create a symbiosis between man and machine. His work in the Pentagon and his research funding for interactive computing and worldwide networks led to the development of the standalone computer with a mouse and graphical user interface. His background in psychology was key to understanding how computers could be optimized for humans. His treatises on human/computer symbiosis shifted understanding of what computers were and could be. He is credited with inspiring the next generation of computing researchers, and his legacy is seen today in the internet and other technologies.

- Highlights first synced by [[Readwise]] [[Feb 21st, 2023]]
	- Claude Shannon thought of information through a 5 part framework: source, transmitter, communication medium, receiver, destination. This simple framework helped him think through the purpose of information and not get bogged down in details. Information ought to measure how much you learn from a given message. If you knew everything in a given message, the information content is zero. However, information and meaning is separated as it relates to computers. Shannon also proved that it is possible to get a message through with perfect fidelity no matter how much static or distortion or how faint the signal. It’ll eventually get too slow and the codes too long but it is possible to overcome noise. This is the fundamental theorem of information theory. Shannon didn’t like how information and meaning could be too easily confused so he had Von Neumann come up with a new name and he came up with one immediately: entropy. Information is entropy. It has the same formula as the physicists formula for entropy. A mathematical variable related to the flow of heat. Information is everywhere and in everything it is as old as time and ties together the mind-body problem, computation, communication, and more ([View Highlight](https://read.readwise.io/read/01gsmjvngvgsxfcxscje8pqm0w))
		- **Tags**: #[[information theory]]
		- **Note**: Claude Shannon developed a five-part framework to help him understand information, which consists of a source, transmitter, communication medium, receiver, and destination. Shannon also proved that it is possible to overcome noise when sending a message, laying the foundation for information theory. Shannon and Von Neumann then renamed Shannon's theory to entropy, which is a mathematical variable related to the flow of heat, and is seen as a key concept in connecting together the mind-body problem, computation, communication, and more.