title:: Reinforcement Learning From Human Feedback: From Zero to chatGPT (highlights)
author:: [[HuggingFace]]
full-title:: "Reinforcement Learning From Human Feedback: From Zero to chatGPT"
category:: #articles
url:: https://www.youtube.com/watch?v=2MBJOuVq380
document_note:: This document discusses the use of reinforcement learning to allow machines to learn from human feedback. It looks at how machines can be trained on data sets to generate responses to questions and also how machines can be used to summarize text. It also discusses how OpenAI has used reinforcement learning with human feedback, as well as how to use reward models to train language models. Finally, it looks at the broader implications of using human feedback and the potential of machine learning in a variety of contexts.
tags:: #[[GPT]] #[[rlhf]]

- Highlights first synced by [[Readwise]] [[Apr 13th, 2023]]
	- reinforcement learning is amathematical framework when you hear RLyou should think about this is kind oflike a set of math problems that we'relooking at that are constrained and in
	  
	  this framework we can study a lot ofdifferent interactions in the world sosome terminology that we'll revisitagain and again is that there's an agentinteracting with an environment and theagent interacts with the environment bytaking an action and then theenvironment returns two things calledthe state and the reward the reward isthe objective that we want to optimizeand the state is just kind of arepresentation of the world at thatcurrent time index and the agent usessomething called a policy to map fromthat state to an action ([View Highlight](https://read.readwise.io/read/01gxhn0t0f7h1w5xh9ywg11k4k))
		- **Note**: Agent interacts with environment to optimize reward.
	- it's very
	  
	  open-ended learning so the agent justsees these reward signals and learns howto optimize them over time irrespectiveof the source of the actual signal forthe reward so it's actually this is whya lot of people are drawn to it isbecause it is this ability to create anagent that'll learn to solve complexproblems and this is kind of where westarted talking about rlhs which is thatwe want to use reinforcement learning tosolve this open-ended problem of whatare these hard loss functions that wewant to model ([View Highlight](https://read.readwise.io/read/01gxhn58d6r2z98hp451tghjqe))
		- **Note**: Reinforcement Learning used to solve complex problems.
	- lhf really originated in decisionmaking and this was before deepreinforcement learning when people werecreating autonomous agents that didn'tuse neural networks to represent a valuefunction didn't use neural networks as apolicyand what this did was a machine LearningSystem that kind of created a policy byhaving humanslabel the actions that an agent took asbeing kind of correct or incorrect
	  
	  excuse me and this was just a simpledecision rule where humans labeled everyaction as good or bad and this wasessentially a reward model and a policyput togetherand this paper they introduced thisTamer framework ([View Highlight](https://read.readwise.io/read/01gxhn95vyyfqk2ptvqcy7kma1))
		- **Note**: Humans label actions as good/bad, creating a reward model and policy.
	- open AI is supposedly spending tons ofmoney on the human annotation budget soorders of magnitude more than thesubmaration summarization paper or theseacademics Works they were doing in thepast so they hire a bunch of people towrite these annotations ([View Highlight](https://read.readwise.io/read/01gxhnh55aqsxctab9gvwcnvt4))
	- withlanguage model pre-training so NLP sendsthe Transformer paper has really beentransformed oh that was a rough sentencebut NLP has really taken off with these
	  
	  kind ofstandardized practices for getting alanguage model which is they'll scrapedata from the internet they'll useunsupervised sequence prediction andthese very large models are becomingreally incredible at generating sequenceof sequences of text to mirror thedistributionum that was given to it by this kind ofhuman training Corpus and in our lhfthere's really not a single best answeron what the model size should be ([View Highlight](https://read.readwise.io/read/01gxhnpkfdxep8d3r2nmp01k89))
		- **Note**: This text is discussing the advances in Natural Language Processing (NLP) and how language models are now being pre-trained to generate sequences of text, using data scraped from the internet and unsupervised sequence prediction. It mentions that while there is no single answer on what the model size should be, large models are becoming very effective at mirroring the distribution of the human-trained corpus.
	- generally there's this importantoptional step which is a company can payhumans to write responses toum these kind of important questions orto important prompts that's identifiedand these responses will be really highquality training data where they cancontinue to train this initial languageModel A little bit more some papersrefer to this as supervised fine-tuningsft and kind of one way to think about
	  
	  this is that it's like a high qualityparameter initialization for the rlhfprocess that'll come laterand this is really expensive to dobecause you have to hire people that arerelatively focused to actually writein-depth responses ([View Highlight](https://read.readwise.io/read/01gxhnw0tmp4ajyt94tcd1p7a6))
		- **Note**: This text discusses the optional step of paying humans to write responses to important questions or prompts in order to create high quality training data for an initial language model. This is referred to as supervised fine-tuning (SFT), and is expensive to do since it requires people with the ability to write in-depth responses.
	- the best practices arenot that well known but in reality theseprompt data sets will be orders ofmagnitude smaller than the like textcorpuses used to pre-train a languagemodel because really it's just trying toget at a more specific notion of like atype of text that is really human andinteractive rather than everything onthe internet which everyone knows can bevery noisy and kind of hard to work withand then what happens is that
	  
	  will generate this text and then thedownstream goal of having text is torate the goal is to rank it so what willhappen is you'll pass these promptsthrough a language model or in somecases it's actually multiple languagemodels so if you think about it if youhave multiple models it can kind of belike players in a chess tournament andwhat you'll do is you'll have the sameprompt go through each model that willgenerate different texts and then what ahuman can do is they can label thosedifferent texts and kind of create arelative ranking of what is going on so
	  
	  that's what we're going to do is likethe goal is to try to take thisgenerated text and pass it through someblack box and then have that output besomething that can transform betransformed into a scalar so there'smultiple ways that this can be done someof them are like the ELO method whereyou have head-to-head rankings there'splenty of different ways that can dothis but essentially it's a very humancomponent where a human is using someinterface to then map the text to adownstream scoreand then once we have kind of
	  
	  we have a we need to think about theinput and output pairs for training amodel with supervised learning and whatwe'll do is we'll actually train on asequence of text and it'll take that asan input it'll decode it do Transformermodel things and then the output will betrained on a specific scalar value forrewardand then we'll kind of get this thingthat we call the reward or preferencemodel ([View Highlight](https://read.readwise.io/read/01gxhp11nnrwj6c0pqrqe2et8f))
		- **Note**: Reinforcement Learning from Human Feedback offers a way to generate text using a language model pre-trained on smaller, more specific datasets of human-generated text. Humans then rate the generated text and rank it, creating a relative ranking. This text is then passed through a black box, which transforms it into a scalar score. The input and output pairs are then used to train a model with supervised learning. The output is trained on a specific scalar value for reward, resulting in a reward or preference model.
	- let mebreak down kind of the few common stepsin this kind of iterative loop so whathappens is we take some profit somethingthe user may have said or something wewant the model to be able to generatewell for and we pass that through whatis going to become our policy which is atrained large language model thatgenerates some text and we can pass thattext into the trained reward model and
	  
	  get some scalar value outthat's kind of the core of the systemand we need to put that into a feedbackloop so we can update it over time butthere's really a lot a few moreimportant stepsone of them that people have used thatactually all the popular papers haveused some variation of is to use acallback Library Divergence the KLDivergence was really popular in machinelearning in reality it's a distancemetric between distributionsto not get too into the details of howsampling from a language model works
	  
	  but what happens is that when you passin a prompt the language model generatesa Time sequence a distribution that'sover time and we can look at thosedistributions relative to each other andwhat is going on here is that we'retrying to constrain the policy thislanguage model on the right we're tryingto constrain this policy as we iterateit over time to not be too far from theinitial language model that we knew wasa pretty accurate text descriptor thefailure mode that this present preventsis that the language model could output
	  
	  gibberish to get high reward from thereward modelbut we also want it to get high rewardand be giving out useful text so thisconstraint kind of Keeps Us in theoptimization landscape that we want tobe inthere's a note that deepminddoesn't use this in the reward but theyrather apply it in the actual updaterule of the RL algorithmso common theme the implementationdetails vary but the ideas are oftensimilar
	  
	  so now we have this reward model outputand this KL Divergence constraint on thetext what happens is we just combine thescalar notion of reward with a scalingfactor in Lambda just to kind of say howmuch do we care about the reward fromthe reward model versus how much do wecare about the tail constraintand in reality there's options to addeven more inputs to the summation wherefor example instruct GPT adds a rewardterm for the text outputs of the trained
	  
	  model that's getting this iterativeupdate to match some of these highquality annotations that they paid theirhuman annotators to write up forspecific prompts so again it'd be kindof matching that summarization that thehuman wrote up about the grad schoolquestion they want to make sure the textmatches all the human text that theyhave access tobut that's really reliant on data so noteveryone has done this stepand then finally what happens is we plug
	  
	  this reward into a RL Optimizer andgenerally the RL Optimizer will justoperate as if the reward was given to itfrom the environment and then we have atraditional RL Loop where a languagemodel is policy this kind of rewardmodel and text sampling technique is theenvironment and we get the state andreward back out and the RL update rulecan work there's some tricks to it thatlike this RL policy may have someparameters Frozen to help make the
	  
	  optimization landscape more tractablebut in reality that's like it kind of isjust applying PPO which is a policygrading added algorithm onto thelanguage modelso it's A Brief Review PPO stands forproximal policy optimizationwhich is a relatively old on policyreinforcement learning algorithm onpolicy means that as of active data ispassed through the systemthe gradients are computed with respectto that only and rather than keeping a
	  
	  replay buffer of recent transitions PPOworks on discrete or continuous actionswhich is why it can work okay withlanguage it's been around for a longtimeum which really means that it's kind ofoptimized for those parallel parallelapproach which has been really importantbecause these language models are waybigger than any reinforceable learningpolicies would use ([View Highlight](https://read.readwise.io/read/01gxhp8cx9a7fbx56nxxp16bm9))
		- **Note**: Reinforcement Learning from Human Feedback is an iterative process in which a language model is used to generate text, the output of which is then passed into a trained reward model to generate a scalar value. This value is then combined with a constraint from the KL Divergence to ensure the language model is not generating gibberish to get high rewards, as well as additional reward terms from human annotations. Finally, the reward is fed into a PPO (Proximal Policy Optimization) algorithm, which is used to update the language model.