title:: Here’s a Hot Take: Fine-Tuning Your Own LLM’s on Top... (highlights)
author:: [[Jerry Liu]]
full-title:: "Here’s a Hot Take: Fine-Tuning Your Own LLM’s on Top..."
category:: #articles
url:: https://twitter.com/jerryjliu0/status/1593283660702048256
document_note:: Alexandr Wang and Alex Tamkin suggest that companies should not solely focus on fine-tuning their own language models on GPT-3/4/N to create a differentiable data moat, as there are other ways to interact with external knowledge sources. They emphasize GPT's ability to reason and interact with external data, using tools such as Chain-of-Thought, GPT Index, Text to SQL and Text to Excel, as well as transformers. They also suggest that companies should think of GPT as a reasoning engine and data processor instead of just a memorizer. Finally, they suggest that companies should build data flywheels to prevent commoditization.
tags:: #[[gpt]]

- Highlights first synced by [[Readwise]] [[Feb 25th, 2023]]
	- There's lots of great research/tools exploring GPT’s ability to reason and interact with external data: chain-of-thought ([View Highlight](https://read.readwise.io/read/01gt3jhmwkqg8gsg0qy9cz24em))
	- GPT is in its ability to REASON, not memorize more world knowledge. ([View Highlight](https://read.readwise.io/read/01gt3jdfvx331bejmkz5kxxshp))
	- Software was built on top of a CPU and memory/hard drive; differentiated apps can be built on GPT as a reasoning engine + data. ([View Highlight](https://read.readwise.io/read/01gt3jh5s314xn59p96r6apzkg))