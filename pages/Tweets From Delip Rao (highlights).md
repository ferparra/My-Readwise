title:: Tweets From Delip Rao (highlights)
author:: [[@deliprao on Twitter]]
full-title:: "Tweets From Delip Rao"
category:: #tweets
url:: https://twitter.com/deliprao

- Highlights first synced by [[Readwise]] [[Nov 19th, 2022]]
	- Many hard AI problems can be simplified with a "dual" UX problem. For e.g., entity linking in the wild is a super hard problem, but building an autocomplete feature for user inputs makes the question irrelevant/"solved". Software 2.0 camp misses this & leads to inefficiencies. ([View Tweet](https://twitter.com/search?q=Many%20hard%20AI%20problems%20can%20be%20simplified%20with%20a%20%22dual%22%20UX%20problem.%20For%20e.g.%2C%20entity%20linking%20in%20the%20wild%20is%20a%20super%20hard%20problem%2C%20but%20building%20an%20autocomplete%20feature%20for%20user%20inputs%20makes%20the%20question%20irrelevant/%22solved%22.%20Software%202.0%20camp%20m%20%28from%3A%40deliprao%29))
	- At 2100+ (not a typo) pages ðŸ¤¯, this is almost all the Math you need for Machine Learning without dumbing it down! PDF link: https://t.co/gXJosvzZLS 
	  
	  ![](https://pbs.twimg.com/media/FfZIE2zWIAAQtN3.jpg) ([View Tweet](https://twitter.com/deliprao/status/1582531571394916352))
- New highlights added [[Jan 9th, 2023]] at 12:45 AM
	- History class: This 2015 paper is the mother of all LM based pre-training approaches, including the GPT, but few are aware of it. GPT (Radford et al 2018) was a direct application of Transformers (Vasvani et al 2017) to the result in this paper (w/ lot of work & insight ofc). 
	  
	  ![](https://pbs.twimg.com/media/Fl6Zko_XgAA9qPC.png) ([View Tweet](https://twitter.com/deliprao/status/1611896130589057025))