title:: 103. Gillian Hadfield - How to Create Explainable AI Regulations That Actually Make Sense (highlights)
author:: [[Towards Data Science]]
full-title:: "103. Gillian Hadfield - How to Create Explainable AI Regulations That Actually Make Sense"
category:: #podcasts

- Highlights first synced by [[Readwise]] [[Nov 19th, 2022]]
	- The Different Layers of Explanation
	  
	  Summary:
	  There's both different layers and different purposes for explanation. The challenge here is that our machine learning models are increasingly massive. It's really not very easy to say, well, why did the model put the label of a cat on that image? Or why did it score that particular application for credit? So explanations like that that help you understand how does your system producing can help you make up a safer system.
	  
	  Transcript:
	  Speaker 2
	  Yeas i imagine when you talk about, like, a defense, military, technical application of a i, and you say, like, ok, explain, maybe explain this drone's decision to pull the trigger and shoot this person, there's, there's one level of explanation that comes from that. Well, ok? It it happened because, you know, nuron number 175 fired, and it fired because these other nurons fired, and so on. Which is a very kind of reductive explanation. But then there's another level that's throgh more practicaln wellit fired because this person had these features that we correlated with the appearance of some member of the taliban or something, and so we pulled the trigger. And then an even higher level still is is like, well, it fired because the, you know, thh, the private who was piloting the drone, gave license to the algrthm to make this decision. It seems like the wreall these different layers of explanation. How do you think about which layer applies, and how do you think governments are thinking about that in the context of this legislation?
	  
	  Speaker 1
	  Yes, itt's really important to to really focus on the idea that there's both different layers and different purposes for explanation. And it's the purpose you want to put the explanation to that really we should be thinking about when we're saying, what do we mean by an explanation? Now, maybe it helps to have a little bit of background here for any listeners who don't kind tof know all the details machine learning, and why is this a challenge a, but, you know, just as jus to get everybody up on the same page here. The challenge here is that our machine learning models are increasingly massive, ah, and they consist of, you know, thousands, millions, in the case of big language models, billions of parimeters. And it's really not so they get this kits referred to, as, you know, as the black box problem. It's really not very easy to say, well, why did that? Why did the model put the label of a cat on that image? Or why did, why did it score that particular application for credit? Alo, to understand how did the model work? If you had a really simple mathematical model, you could, right? You its had like five variables in it. A, you say, well, there's the coefficient n on age that's edon. We can tell exactly why it behaved that way. A, so the first type of explanation people are talking about, and this is the one that computer scientists are thinking about, and say, you know, darpa, when they started talking about explainablei a, they're talking about the cause account. How? How did the math transform that input into this output? And that's a very challenging thing to do when you've got these big, big, complex models. But it's the kind of thing you'd want to know in order to say ok, why does it make mistakes? Maybe you'll learn that, you know, oh, the reason it labelled that picture a you know, there's an example that gets talked about ofte you know, people who trained a model to tell the difference between pictures of huskies and wolvesdo reson anybody in talk about yes, exactly. You give away the punch lide there. That leer. That's right. That's right. So train a model to tell thet etween between huskies and wolves. You get great accuracy on your test set. And and then you go and you look for an explanation of how why it made a mistake in one case, and discover, actually, you trained a model to recognize snow, because all your pictures of wolves were on snow, and all the pictures of dogs were on grass or dirt or rock or something a. So explanations like that that help you understand how does your, how is your system producing, can help you make up a safer system, can help you reduce errors, can give you more, you know, a more robust system than when you send it out into the real world. It's actually behaving the way you expect id to. So there's that kind of explanation. It's a quite important. But if we go back to the g d p r an and think about automated decision making, when they say, you, citizen, resident data owner, have a right to an explanation about how your data was used to reach an automated decision, most of those people are not at all interested in the causal story of, you know, what neurons fired or, ah, you know, what weight was attached to what feature. Ah, they i think what they want is what i call a justification. And i actually advocate for us to start using that term. That really the g p r should say, you have a right to a justification of a decision made using a using using od made decision process. Because what what that person is looking for is like what our legal systems give us the right to say, hey, you didn't give me lone or you denied me that job, and i don't i think you violated the rules when you did that. You didn't look at my application carefully. You didn't take into account the material i sent youa you discriminated against me on a prohibited ground. That's really what we mean. I think when you see this impulse from the european parliament to say, all we need explanations. It's no, people have a right to a, be given an account of why the decision made by the machine is acceptable, given all the rules we have about nd, the standards and norms we have around how we make decisions.
	  
	  Speaker 2
	  And it's part of that distinction. ([TimeÂ 0:07:52](https://share.snipd.com/snip/f326cdac-89d0-455b-8172-8b4b1102efb9))