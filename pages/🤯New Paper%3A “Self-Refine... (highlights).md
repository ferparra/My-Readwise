title:: ğŸ¤¯New Paper: â€œSelf-Refine... (highlights)
author:: [[@peteskomoroch on Twitter]]
full-title:: "ğŸ¤¯New Paper: â€œSelf-Refine..."
category:: #tweets
url:: https://twitter.com/peteskomoroch/status/1642721632144990210

- Highlights first synced by [[Readwise]] [[Apr 7th, 2023]]
	- ğŸ¤¯New paper: â€œSelf-Refine: Iterative Refinement with Self-Feedbackâ€ shows LLMs can improve themselves without humans.
	  
	  â€œSELF-REFINE is unique in that it operates within a single LLM, requiring neither additional training data nor reinforcement learning.â€ https://t.co/8Coo3T66kZ 
	  
	  ![](https://pbs.twimg.com/media/FswenaWakAEBoFs.jpg) 
	  
	  ![](https://pbs.twimg.com/media/FswenaTacAEqKbU.jpg) 
	  
	  ![](https://pbs.twimg.com/media/FswenaWacAAh1zw.jpg) ([View Tweet](https://twitter.com/peteskomoroch/status/1642721632144990210))
		- **Note**: Thread
	- I have a BS in math/physics, had a career in machine learning, and worked at one of the largest social networks during early hyper-growth. I never fully appreciated power laws until today. If we say something follows a power law, we mean â€œthe bigger it gets, the faster it growsâ€. ([View Tweet](https://twitter.com/peteskomoroch/status/1642724339081707521))