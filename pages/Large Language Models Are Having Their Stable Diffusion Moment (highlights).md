title:: Large Language Models Are Having Their Stable Diffusion Moment (highlights)
author:: [[simonwillison.net]]
full-title:: "Large Language Models Are Having Their Stable Diffusion Moment"
category:: #articles
url:: https://simonwillison.net/2023/Mar/11/llama/

- Highlights first synced by [[Readwise]] [[Mar 12th, 2023]]
	- There have been dozens of open large language models released over the past few years, but none of them have quite hit the sweet spot for me in terms of the following:
	  
	  •   Easy to run on my own hardware
	  •   Large enough to be useful—ideally equivalent in capabilities to GPT-3
	  •   Open source enough that they can be tinkered with ([View Highlight](https://read.readwise.io/read/01gv9mmtk2kphmajj4gyjxsb9p))
	- We introduce LLaMA, a collection of founda- tion language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla- 70B and PaLM-540B. We release all our models to the research community. ([View Highlight](https://read.readwise.io/read/01gv9mn0ypchfwxncvmk08tj20))
	- LLaMA on its own isn’t much good if it’s still too hard to run it on a personal laptop.
	  
	  Enter [Georgi Gerganov](https://ggerganov.com/).
	  
	  Georgi is an open source developer based in Sofia, Bulgaria (according to [his GitHub profile](https://github.com/ggerganov)). He previously released [whisper.cpp](https://github.com/ggerganov/whisper.cpp), a port of OpenAI’s Whisper automatic speech recognition model to C++. That project made Whisper applicable to a huge range of new use cases. ([View Highlight](https://read.readwise.io/read/01gv9mnft5czx38nt8ywrscr36))