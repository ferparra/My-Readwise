title:: What Is ChatGPT Doing … and Why Does It Work? (highlights)
author:: [[stephenwolfram.com]]
full-title:: "What Is ChatGPT Doing … and Why Does It Work?"
category:: #articles
url:: https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/
document_note:: ChatGPT is a neural network that has been trained to generate human-language text and distinguish between words. It is an artificial intelligence system that is able to “capture the essence” of human language and thinking by relying on neural nets which can “interpolate” and “generalize” between examples. It has 175 billion weights, allowing it to make a “reasonable model” of text humans write. It is able to use words to create “neural-net-friendly” collections of numbers, and it has the potential to use a computational language that is understandable to humans.

- The voodoo begins to creep in when we pick lower-ranked words at random instead of always picking the highest-ranked word.
-
- Highlights first synced by [[Readwise]] [[Feb 16th, 2023]]
	- One might think it should be the “highest-ranked” word (i.e. the one to which the highest “probability” was assigned). But this is where a bit of voodoo begins to creep in. Because for some reason—that maybe one day we’ll have a scientific-style understanding of—if we always pick the highest-ranked word, we’ll typically get a very “flat” essay, that never seems to “show any creativity” (and even sometimes repeats word for word). But if sometimes (at random) we pick lower-ranked words, we get a “more interesting” essay. ([View Highlight](https://read.readwise.io/read/01gsa66fx0jtf8t9a1jxarqpjz))
	- The first thing to explain is that what ChatGPT is always fundamentally trying to do is to produce a “reasonable continuation” of whatever text it’s got so far, where by “reasonable” we mean “what one might expect someone to write after seeing what people have written on billions of webpages, etc.” ([View Highlight](https://read.readwise.io/read/01gs9vweh286kncseby81d86s9))
	- ![](https://content.wolfram.com/uploads/sites/43/2023/02/sw021423img1.png) ([View Highlight](https://read.readwise.io/read/01gs9vxfdfqmgg7dvxnwd0gcrv))
		- **Tags**: #[[llms]] #[[prediction]] #[[ai]]
- New highlights added [[Feb 28th, 2023]] at 9:35 PM
	- ![](https://content.wolfram.com/uploads/sites/43/2023/02/sw021423img42.png) ([View Highlight](https://read.readwise.io/read/01gtbn8sz2syhcbnz00cvhzay8))
		- **Tags**: #[[neural networks]]
	- ![](https://content.wolfram.com/uploads/sites/43/2023/02/sw021423img44.png) ([View Highlight](https://read.readwise.io/read/01gtbn8kw9cwhrfb68r6z4qshw))
		- **Tags**: #[[neural networks]]
	- ![](https://content.wolfram.com/uploads/sites/43/2023/02/sw021423img46.png) ([View Highlight](https://read.readwise.io/read/01gtbn8aeatvgs5z9kbse4s00p))
		- **Tags**: #[[neural networks]]
	- ![](https://content.wolfram.com/uploads/sites/43/2023/02/sw021423img52.png) ([View Highlight](https://read.readwise.io/read/01gtbn9hs3ymazjvsa20y64ns0))
	- ![](https://content.wolfram.com/uploads/sites/43/2023/02/sw021423img60.png) ([View Highlight](https://read.readwise.io/read/01gtbna3n87235db6q54h9j583))
		- **Tags**: #[[neural networks]]
	- ![](https://content.wolfram.com/uploads/sites/43/2023/02/sw021423img61.png) ([View Highlight](https://read.readwise.io/read/01gtbnafm19kwh5m0025ernwbb))
		- **Tags**: #[[neural networks]]
	- At each stage in this “training” the weights in the network are progressively adjusted—and we see that eventually we get a network that successfully reproduces the function we want. So how do we adjust the weights? The basic idea is at each stage to see “how far away we are” from getting the function we want—and then to update the weights in such a way as to get closer.
	  
	  To find out “how far away we are” we compute what’s usually called a “loss function” (or sometimes “cost function”). Here we’re using a simple (L2) loss function that’s just the sum of the squares of the differences between the values we get, and the true values. And what we see is that as our training process progresses, the loss function progressively decreases (following a certain “learning curve” that’s different for different tasks)—until we reach a point where the network (at least to a good approximation) successfully reproduces the function we want: ([View Highlight](https://read.readwise.io/read/01gtbnbenc7yggqvntef18tbsg))
		- **Note**: ChatGPT works by progressively adjusting the weights in the network at each stage of training. The "loss function" compares the values the network produces to the true values, and the loss decreases as training progresses until the network successfully reproduces the desired function.
	- ![](https://content.wolfram.com/uploads/sites/43/2023/02/sw021423img62.png) ([View Highlight](https://read.readwise.io/read/01gtbnapktanrnkpr8h5ec3j0h))
		- **Tags**: #[[neural networks]]
	- Or put another way, there’s an ultimate tradeoff between capability and trainability: the more you want a system to make “true use” of its computational capabilities, the more it’s going to show computational irreducibility, and the less it’s going to be trainable. And the more it’s fundamentally trainable, the less it’s going to be able to do sophisticated computation. ([View Highlight](https://read.readwise.io/read/01gtbndbv43bsch9jrakqgn7s8))
		- **Note**: ChatGPT is attempting to balance the tradeoff between capability and trainability, striving for a system that can do sophisticated computation but is also trainable. This challenge is due to the fact that any increase in capability will reduce trainability, and any increase in trainability will reduce the system's capability.
	- ![](https://content.wolfram.com/uploads/sites/43/2023/02/sw021423img78.png) ([View Highlight](https://read.readwise.io/read/01gtbne74j0hgxscst2thmnvaf))
	- OK, so we’re finally ready to discuss what’s inside ChatGPT. And, yes, ultimately, it’s a giant neural net—currently a version of the so-called GPT-3 network with 175 billion weights. In many ways this is a neural net very much like the other ones we’ve discussed. But it’s a neural net that’s particularly set up for dealing with language. And its most notable feature is a piece of neural net architecture called a “transformer”.
	  
	  In the first neural nets we discussed above, every neuron at any given layer was basically connected (at least with some weight) to every neuron on the layer before. But this kind of fully connected network is (presumably) overkill if one’s working with data that has particular, known structure. And thus, for example, in the early stages of dealing with images, it’s typical to use so-called [convolutional neural nets](https://reference.wolfram.com/language/ref/ConvolutionLayer.html) (“convnets”) in which neurons are effectively laid out on a grid analogous to the pixels in the image—and connected only to neurons nearby on the grid.
	  
	  The idea of transformers is to do something at least somewhat similar for sequences of tokens that make up a piece of text. But instead of just defining a fixed region in the sequence over which there can be connections, transformers instead introduce the notion of “[attention](https://reference.wolfram.com/language/ref/AttentionLayer.html)”—and the idea of “paying attention” more to some parts of the sequence than others. Maybe one day it’ll make sense to just start a generic neural net and do all customization through training. But at least as of now it seems to be critical in practice to “modularize” things—as transformers do, and probably as our brains also do.
	  
	  OK, so what does ChatGPT (or, rather, the GPT-3 network on which it’s based) actually do? Recall that its overall goal is to continue text in a “reasonable” way, based on what it’s seen from the training it’s had (which consists in looking at billions of pages of text from the web, etc.) So at any given point, it’s got a certain amount of text—and its goal is to come up with an appropriate choice for the next token to add.
	  
	  It operates in three basic stages. First, it takes the sequence of tokens that corresponds to the text so far, and finds an embedding (i.e. an array of numbers) that represents these. Then it operates on this embedding—in a “standard neural net way”, with values “rippling through” successive layers in a network—to produce a new embedding (i.e. a new array of numbers). It then takes the last part of this array and generates from it an array of about 50,000 values that turn into probabilities for different possible next tokens. (And, yes, it so happens that there are about the same number of tokens used as there are common words in English, though only about 3000 of the tokens are whole words, and the rest are fragments.)
	  
	  A critical point is that every part of this pipeline is implemented by a neural network, whose weights are determined by end-to-end training of the network. In other words, in effect nothing except the overall architecture is “explicitly engineered”; everything is just “learned” from training data.
	  
	  There are, however, plenty of details in the way the architecture is set up—reflecting all sorts of experience and neural net lore. And—even though this is definitely going into the weeds—I think it’s useful to talk about some of those details, not least to get a sense of just what goes into building something like ChatGPT.
	  
	  First comes the embedding module. Here’s a schematic Wolfram Language representation for it for GPT-2:
	  
	  ![](https://content.wolfram.com/uploads/sites/43/2023/02/sw021423img86.png)
	  
	  The input is a [vector of *n* tokens](https://reference.wolfram.com/language/ref/netencoder/SubwordTokens.\ html) (represented as in the previous section by integers from 1 to about 50,000). Each of these tokens is converted (by a [single-layer neural net](https://reference.wolfram.com/language/ref/EmbeddingLayer.html)) into an embedding vector (of length 768 for GPT-2 and 12,288 for ChatGPT’s GPT-3). Meanwhile, there’s a “secondary pathway” that takes the [sequence of (integer) positions](https://reference.wolfram.com/language/ref/SequenceIndicesLayer.html) for the tokens, and from these integers creates another embedding vector. And finally the embedding vectors from the token value and the token position are [added together](https://reference.wolfram.com/language/ref/ThreadingLayer.html)—to produce the final sequence of embedding vectors from the embedding module.
	  
	  Why does one just add the token-value and token-position embedding vectors together? I don’t think there’s any particular science to this. It’s just that various different things have been tried, and this is one that seems to work. And it’s part of the lore of neural nets that—in some sense—so long as the setup one has is “roughly right” it’s usually possible to home in on details just by doing sufficient training, without ever really needing to “understand at an engineering level” quite how the neural net has ended up configuring itself.
	  
	  Here’s what the embedding module does, operating on the string *hello hello hello hello hello hello hello hello hello hello bye bye bye bye bye bye bye bye bye bye*:
	  
	  ![](https://content.wolfram.com/uploads/sites/43/2023/02/sw021423img87.png)
	  
	  The elements of the embedding vector for each token are shown down the page, and across the page we see first a run of “*hello*” embeddings, followed by a run of “*bye*” ones. The second array above is the positional embedding—with its somewhat-random-looking structure being just what “happened to be learned” (in this case in GPT-2).
	  
	  OK, so after the embedding module comes the “main event” of the transformer: a sequence of so-called “attention blocks” (12 for GPT-2, 96 for ChatGPT’s GPT-3). It’s all pretty complicated—and reminiscent of typical large hard-to-understand engineering systems, or, for that matter, biological systems. But anyway, here’s a schematic representation of a single “attention block” (for GPT-2):
	  
	  ![](https://content.wolfram.com/uploads/sites/43/2023/02/sw021423img88.png)
	  
	  Within each such attention block there are a collection of “attention heads” (12 for GPT-2, 96 for ChatGPT’s GPT-3)—each of which operates independently on different chunks of values in the embedding vector. (And, yes, we don’t know any particular reason why it’s a good idea to split up the embedding vector, or what the different parts of it “mean”; this is just one of those things that’s been “found to work”.)
	  
	  OK, so what do the attention heads do? Basically they’re a way of “looking back” in the sequence of tokens (i.e. in the text produced so far), and “packaging up the past” in a form that’s useful for finding the next token. [In the first section above](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/#its-just-adding-one-word-at-a-time) we talked about using 2-gram probabilities to pick words based on their immediate predecessors. What the “attention” mechanism in transformers does is to allow “attention to” even much earlier words—thus potentially capturing the way, say, verbs can refer to nouns that appear many words before them in a sentence.
	  
	  At a more detailed level, what an attention head does is to recombine chunks in the embedding vectors associated with different tokens, with certain weights. And so, for example, the 12 attention heads in the first attention block (in GPT-2) have the following (“look-back-all-the-way-to-the-beginning-of-the-sequence-of-tokens”) patterns of “recombination weights” for the “*hello*, *bye*” string above:
	  
	  After being processed by the attention heads, the resulting “re-weighted embedding vector” (of length 768 for GPT-2 and length 12,288 for ChatGPT’s GPT-3) is passed through a standard [“fully connected” neural net layer](https://reference.wolfram.com/language/ref/LinearLayer.html). It’s hard to get a handle on what this layer is doing. But here’s a plot of the 768×768 matrix of weights it’s using (here for GPT-2):
	  
	  Taking 64×64 moving averages, some (random-walk-ish) structure b ([View Highlight](https://read.readwise.io/read/01gtbnm98wbbamwzf6p3t288yf))
		- **Note**: ChatGPT is a giant neural net that is set up for dealing with language, using a piece of neural net architecture called a "transformer". This transformer involves attention heads that look back in the sequence of tokens, recombining chunks in the embedding vectors associated with different tokens with certain weights. The output is a vector of probabilities for what token should come next. There are 175 billion calculations done for each token produced, which is why it can take a while to generate a long piece of text with ChatGPT.
	- OK, so we’ve now given an outline of how ChatGPT works once it’s set up. But how did it get set up? How were all those 175 billion weights in its neural net determined? Basically they’re the result of very large-scale training, based on a huge corpus of text—on the web, in books, etc.—written by humans. As we’ve said, even given all that training data, it’s certainly not obvious that a neural net would be able to successfully produce “human-like” text. And, once again, there seem to be detailed pieces of engineering needed to make that happen. But the big surprise—and discovery—of ChatGPT is that it’s possible at all. And that—in effect—a neural net with “just” 175 billion weights can make a “reasonable model” of text humans write.
	  
	  In modern times, there’s lots of text written by humans that’s out there in digital form. The public web has at least several billion human-written pages, with altogether perhaps a trillion words of text. And if one includes non-public webpages, the numbers might be at least 100 times larger. So far, more than 5 million digitized books have been made available (out of 100 million or so that have ever been published), giving another 100 billion or so words of text. And that’s not even mentioning text derived from speech in videos, etc. (As a personal comparison, [my total lifetime output of published material](https://www.stephenwolfram.com/publications/) has been a bit under 3 million words, and over the [past 30 years I’ve written](https://writings.stephenwolfram.com/2012/03/the-personal-analytics-of-my-life/) about 15 million words of email, and altogether typed perhaps 50 million words—and in just the past couple of years I’ve spoken more than 10 million words on [livestreams](https://www.stephenwolfram.com/livestreams). And, yes, I’ll train a bot from all of that.)
	  
	  But, OK, given all this data, how does one train a neural net from it? The basic process is very much as we discussed it in the simple examples above. You present a batch of examples, and then you adjust the weights in the network to minimize the error (“loss”) that the network makes on those examples. The main thing that’s expensive about “back propagating” from the error is that each time you do this, every weight in the network will typically change at least a tiny bit, and there are just a lot of weights to deal with. (The actual “back computation” is typically only a small constant factor harder than the forward one.)
	  
	  With modern GPU hardware, it’s straightforward to compute the results from batches of thousands of examples in parallel. But when it comes to actually updating the weights in the neural net, current methods require one to do this basically batch by batch. (And, yes, this is probably where actual brains—with their combined computation and memory elements—have, for now, at least an architectural advantage.)
	  
	  Even in the seemingly simple cases of learning numerical functions that we discussed earlier, we found we often had to use millions of examples to successfully train a network, at least from scratch. So how many examples does this mean we’ll need in order to train a “human-like language” model? There doesn’t seem to be any fundamental “theoretical” way to know. But in practice ChatGPT was successfully trained on a few hundred billion words of text.
	  
	  Some of the text it was fed several times, some of it only once. But somehow it “got what it needed” from the text it saw. But given this volume of text to learn from, how large a network should it require to “learn it well”? Again, we don’t yet have a fundamental theoretical way to say. Ultimately—as we’ll discuss further below—there’s presumably a certain “total algorithmic content” to human language and what humans typically say with it. But the next question is how efficient a neural net will be at implementing a model based on that algorithmic content. And again we don’t know—although the success of ChatGPT suggests it’s reasonably efficient.
	  
	  And in the end we can just note that ChatGPT does what it does using a couple hundred billion weights—comparable in number to the total number of words (or tokens) of training data it’s been given. In some ways it’s perhaps surprising (though empirically observed also in smaller analogs of ChatGPT) that the “size of the network” that seems to work well is so comparable to the “size of the training data”. After all, it’s certainly not that somehow “inside ChatGPT” all that text from the web and books and so on is “directly stored”. Because what’s actually inside ChatGPT are a bunch of numbers—with a bit less than 10 digits of precision—that are some kind of distributed encoding of the aggregate structure of all that text.
	  
	  Put another way, we might ask what the “effective information content” is of human language and what’s typically said with it. There’s the raw corpus of examples of language. And then there’s the representation in the neural net of ChatGPT. That representation is very likely far from the “algorithmically minimal” representation (as we’ll discuss below). But it’s a representation that’s readily usable by the neural net. And in this representation it seems there’s in the end rather little “compression” of the training data; it seems on average to basically take only a bit less than one neural net weight to carry the “information content” of a word of training data.
	  
	  When we run ChatGPT to generate text, we’re basically having to use each weight once. So if there are *n* weights, we’ve got of order *n* computational steps to do—though in practice many of them can typically be done in parallel in GPUs. But if we need about *n* words of training data to set up those weights, then from what we’ve said above we can conclude that we’ll need about *n*2 computational steps to do the training of the network—which is why, with current methods, one ends up needing to talk about billion-dollar training efforts. ([View Highlight](https://read.readwise.io/read/01gtbp44b1hs8wfdkkg9551kxc))
		- **Note**: ChatGPT is a neural network with 175 billion weights, which is trained on a huge corpus of text written by humans found on the web, in books, etc. It was discovered that it is possible to create a "reasonable model" of human-written text with such a neural net. The training process involves presenting batches of examples and adjusting the weights in the network to minimize the error made. There is a lot of text written by humans that is available in digital form, which can be used to train a neural net. The process requires millions of examples, and ChatGPT was successfully trained on a few hundred billion words of text. The success of ChatGPT suggests that it is reasonably efficient in implementing a model based on the "algorithmic content" of human language. It has been concluded that it takes about one neural net weight to carry the "information content" of a word of training data, and using current methods, a billion-dollar training effort is needed.
	- The success of ChatGPT is, I think, giving us evidence of a fundamental and important piece of science: it’s suggesting that we can expect there to be major new “laws of language”—and effectively “laws of thought”—out there to discover. ([View Highlight](https://read.readwise.io/read/01gtbnjgfqjh038hye924q6mcx))
	- ![](https://content.wolfram.com/uploads/sites/43/2023/02/sw021423img96.png) ([View Highlight](https://read.readwise.io/read/01gtbnfhanxxegx5j154kf3wqg))
		- **Tags**: #[[gpt]]