title:: Before I Actually Start... (highlights)
author:: [[@johncarlosbaez on Twitter]]
full-title:: "Before I Actually Start..."
category:: #tweets
url:: https://twitter.com/johncarlosbaez/status/1544385525443768321

- Highlights first synced by [[Readwise]] [[Nov 19th, 2022]]
	- Before I actually start explaining entropy, a warning:
	  
	  It can be hard to learn about entropy at first because there are many kinds - and people often don't say which one they're talking about.   
	  
	  Here are 5 kinds.  Luckily, they are closely related!
	  
	  (1/n) 
	  
	  ![](https://pbs.twimg.com/media/FW7BNRJVUAAFkVB.jpg) ([View Tweet](https://twitter.com/johncarlosbaez/status/1544385525443768321))
		- **Note**: Thread
	- In thermodynamics we primarily have a formula for the *change* in entropy: if you add an infinitesimal amount of heat to a system, this equals T dS where T is the temperature and dS is the infinitesimal change in entropy.   Rather indirect!
	  
	  (2/n)
	  
	  https://t.co/PRiWMlwhio ([View Tweet](https://twitter.com/johncarlosbaez/status/1544386310386749440))
	- In classical statistical mechanics, Gibbs explained entropy in terms of a probability distribution p on the space of states of a classical system.   He showed that entropy is the integral of -p ln(p) times a constant called Boltzmann's constant.
	  
	  (3/n)
	  
	  https://t.co/Xuk4KzE2Kn ([View Tweet](https://twitter.com/johncarlosbaez/status/1544387437362757632))
	- Later von Neumann generalized Gibbs' formula for entropy from classical to *quantum* statistical mechanics!   He replaced the probability distribution p by a so-called density matrix œÅ, and the integral by a trace.   
	  
	  (4/n)
	  https://t.co/806CBwYEsm ([View Tweet](https://twitter.com/johncarlosbaez/status/1544388807847337986))
	- Later Shannon invented a formula for the entropy of a random string of symbols, often called the 'Shannon entropy'.  
	  
	  But it's just a special case of Gibbs' formula for entropy in classical statistical mechanics (without the Boltzmann's constant).
	  
	  (5/n)
	  
	  https://t.co/CxHQx92PKC ([View Tweet](https://twitter.com/johncarlosbaez/status/1544390065853308930))
	- Later Kolmogorov invented a formula for the entropy of a *specific* string of symbols.  It's just the length of the shortest program, written in bits, that prints out this string.   It depends on the computer language, but not too much to be useful.
	  
	  (6/n)https://t.co/gqOHzdIndN ([View Tweet](https://twitter.com/johncarlosbaez/status/1544391232389951488))
	- There's a network of theorems connecting all these 5 concepts of entropy.  But for now I'm only going to explain Gibbs' concept of entropy for classical statistical mechanics - starting from Shannon's special case, so I can clearly link entropy to information.
	  
	  (7/n, n = 7) ([View Tweet](https://twitter.com/johncarlosbaez/status/1544392276616458246))