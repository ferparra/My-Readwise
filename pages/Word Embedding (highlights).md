title:: Word Embedding (highlights)
author:: [[wikipedia.org]]
full-title:: "Word Embedding"
category:: #articles
url:: https://en.wikipedia.org/wiki/Word_embedding
tags:: #[[ai]]

- Highlights first synced by [[Readwise]] [[Jan 5th, 2023]]
	- In [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing) (NLP), a **word embedding** is a representation of a word. The [[embedding]] is used in text analysis. Typically the representation is a real-valued vector that encodes the meaning of the word in such a way that words that are closer in the [[vector space]] are expected to be similar in meaning.[[1]](https://en.wikipedia.org/wiki/Word_embedding#cite_note-1) Word [[embeddings]] can be obtained using [language modeling](https://en.wikipedia.org/wiki/Language_model) and [feature learning](https://en.wikipedia.org/wiki/Feature_learning) techniques where words or phrases from the vocabulary are mapped to [vectors](https://en.wikipedia.org/wiki/Vector_(mathematics)) of [real numbers](https://en.wikipedia.org/wiki/Real_numbers). ([View Highlight](https://read.readwise.io/read/01gp088vzm3bexgn7cy73kwy22))
- New highlights added [[Apr 7th, 2023]] at 3:38 PM
	- Polysemy and homonymy
	  
	  Historically, one of the main limitations of static word embeddings or word [vector space models](https://en.wikipedia.org/wiki/Vector_space_model) is that words with multiple meanings are conflated into a single representation (a single vector in the semantic space). In other words, [polysemy](https://en.wikipedia.org/wiki/Polysemy) and [homonymy](https://en.wikipedia.org/wiki/Homonym) are not handled properly. For example, in the sentence "The club I tried yesterday was great!", it is not clear if the term *club* is related to the word sense of a *[club sandwich](https://en.wikipedia.org/wiki/Club_sandwich)*, *[baseball club](https://en.wikipedia.org/wiki/Baseball)*, *[clubhouse](https://en.wikipedia.org/wiki/Meeting_house)*, *[golf club](https://en.wikipedia.org/wiki/Golf_club)*, or any other sense that *club* might have. The necessity to accommodate multiple meanings per word in different vectors (multi-sense embeddings) is the motivation for several contributions in NLP to split single-sense embeddings into multi-sense ones.[[30]](https://en.wikipedia.org/wiki/Word_embedding#cite_note-30)[[31]](https://en.wikipedia.org/wiki/Word_embedding#cite_note-31)
	  
	  Most approaches that produce multi-sense embeddings can be divided into two main categories for their word sense representation, i.e., unsupervised and knowledge-based.[[32]](https://en.wikipedia.org/wiki/Word_embedding#cite_note-32) Based on [word2vec](https://en.wikipedia.org/wiki/Word2vec) skip-gram, Multi-Sense Skip-Gram (MSSG)[[33]](https://en.wikipedia.org/wiki/Word_embedding#cite_note-33) performs word-sense discrimination and embedding simultaneously, improving its training time, while assuming a specific number of senses for each word. In the Non-Parametric Multi-Sense Skip-Gram (NP-MSSG) this number can vary depending on each word. Combining the prior knowledge of lexical databases (e.g., [WordNet](https://en.wikipedia.org/wiki/WordNet), [ConceptNet](https://en.wikipedia.org/wiki/Open_Mind_Common_Sense), [BabelNet](https://en.wikipedia.org/wiki/BabelNet)), word embeddings and [word sense disambiguation](https://en.wikipedia.org/wiki/Word_sense_disambiguation), Most Suitable Sense Annotation (MSSA)[[34]](https://en.wikipedia.org/wiki/Word_embedding#cite_note-34) labels word-senses through an unsupervised and knowledge-based approach considering a word's context in a pre-defined sliding window. Once the words are disambiguated, they can be used in a standard word embeddings technique, so multi-sense embeddings are produced. MSSA architecture allows the disambiguation and annotation process to be performed recurrently in a self-improving manner.
	  
	  The use of multi-sense embeddings is known to improve performance in several NLP tasks, such as [part-of-speech tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging), semantic relation identification, [semantic relatedness](https://en.wikipedia.org/wiki/Semantic_relatedness), [named entity recognition](https://en.wikipedia.org/wiki/Named_entity_recognition) and sentiment analysis.[[35]](https://en.wikipedia.org/wiki/Word_embedding#cite_note-:1-35)[[36]](https://en.wikipedia.org/wiki/Word_embedding#cite_note-36) ([View Highlight](https://read.readwise.io/read/01gxc9qfhv57kcnp6dnpv1r89j))
		- **Note**: Static word embeddings or vector space models are not very good at understanding words with more than one meaning. For example, the word "club" can mean a sandwich, a sports team, or a place to hang out. In the sentence "The club I tried yesterday was great!", it is not clear which meaning of "club" is being used. To fix this problem, researchers have been working on creating multi-sense embeddings, which are words with more than one meaning assigned to different vectors (representations) in a semantic space. This allows us to better understand which meaning of a word is being used in a sentence. Multi-sense embeddings have been shown to help with tasks such as figuring out which part of speech a word is, understanding the relationships between words, and recognizing named entities.