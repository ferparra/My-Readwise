title:: More Efficient In-Context Learning with GLaM (highlights)
author:: [[Google AI Blog]]
full-title:: "More Efficient In-Context Learning with GLaM"
category:: #articles
url:: https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html
tags:: #[[LLMs]]

- Highlights first synced by [[Readwise]] [[Apr 7th, 2023]]
	- Large language models (e.g., [GPT-3](https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)) have many significant capabilities, such as performing [few-shot learning](https://en.wikipedia.org/wiki/One-shot_learning) across a wide array of tasks, including reading comprehension and question answering with very few or no training examples. While these models can perform better by simply using more parameters, training and serving these large models can be very computationally intensive. Is it possible to train and use these models more efficiently? ([View Highlight](https://read.readwise.io/read/01gx9s6s9hce17n14th1a4b92z))
	- Similar to the [GShard MoE Transformer](https://openreview.net/pdf?id=qrwe7XHTmYb), we replace the single [feedforward network](https://en.wikipedia.org/wiki/Feedforward_neural_network) (the simplest layer of an artificial neural network, “Feedforward or FFN” in the blue boxes) of every other transformer layer with a MoE layer. This MoE layer has multiple experts, each a feedforward network with identical architecture but different weight parameters. Even though this MoE layer has many more parameters, the experts are sparsely activated, meaning that for a given input token, only two experts are used, giving the model more capacity while limiting computation. During training, each MoE layer's *gating network* is trained to use its input to activate the best two experts for each token, which are then used for inference. For a MoE layer of *E* experts, this essentially provides a collection of *E*×(*E*-1) different feedforward network combinations (instead of one as in the classic [Transformer](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) architecture), leading to more computational flexibility. ([View Highlight](https://read.readwise.io/read/01gx9s6764q8m4c071k0k9abr5))