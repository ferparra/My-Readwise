title:: 🚀 1/ Excited to Share Ou... (highlights)
author:: [[@MikhailBurtsev on Twitter]]
full-title:: "🚀 1/ Excited to Share Ou..."
category:: #tweets
url:: https://twitter.com/MikhailBurtsev/status/1649452141084397569

- Highlights first synced by [[Readwise]] [[Apr 27th, 2023]]
	- 🚀 1/ Excited to share our (with Aydar Bulatov and @yurakuratov ) report on scaling Recurrent Memory Transformer to 2M (yes, two millions)😮 tokens!  🧠🌐 #AI #NLP  #DeepLearning 
	  
	  ![](https://pbs.twimg.com/media/FuQBsX1WYAAS0-s.jpg) ([View Tweet](https://twitter.com/MikhailBurtsev/status/1649452141084397569))
		- **Note**: Thread
	- 2/ 📈 We've tackled the quadratic complexity of attention in  #Transformers by combining token-based memory & segment-level  recurrence, using RMT.
	  🔸 RMT adapts to any Transformer family model 
	  🔸 Memory tokens provide the recurrent connection 🎛️💡 #AI #NLP #DeepLearning 
	  
	  ![](https://pbs.twimg.com/media/FuQC8czXgAgHN7g.png) ([View Tweet](https://twitter.com/MikhailBurtsev/status/1649452145148657684))
	- 3/ 🧠 We tested RMT's memorization capabilities with synthetic datasets requiring fact memorization, detection, & reasoning. The model must separate facts from irrelevant text and use them to answer questions in a 6-class classification. 🎯 #AI #NLP #DeepLearning 
	  
	  ![](https://pbs.twimg.com/media/FuQENh2XgAIJNQQ.jpg) ([View Tweet](https://twitter.com/MikhailBurtsev/status/1649452148118155265))
	- 4/ 📊 In our experiments, we used the pretrained BERT model as the backbone for RMT. We employed curriculum learning, starting with shorter tasks & increasing length upon convergence. This improved accuracy & stability in our model's performance. 💪 #AI #NLP #DeepLearning 
	  
	  ![](https://pbs.twimg.com/media/FuQEgeTXgAQI7lQ.jpg) ([View Tweet](https://twitter.com/MikhailBurtsev/status/1649452152513875968))
	- 5/ 📈 RMT's extrapolation abilities: Models trained on 7 segments  generalize surprisingly well  even on sequences up to 2,043,904 tokens! 🔝🚀 #AI #NLP #DeepLearning 
	  
	  ![](https://pbs.twimg.com/media/FuQE5QrXgA4DLId.jpg) ([View Tweet](https://twitter.com/MikhailBurtsev/status/1649452160617250832))
	- 6/ 🍃 Computational efficiency: RMT scales linearly for any model size  with fixed segment length. Larger Transformers exhibit slower quadratic  scaling, but RMT requires fewer FLOPs and can reduce FLOPs by up to  295x! 🌟✂️ #AI #NLP #DeepLearning #Efficiency 
	  
	  ![](https://pbs.twimg.com/media/FuQFQIQXgA4AtPS.jpg) ([View Tweet](https://twitter.com/MikhailBurtsev/status/1649452164375347208))
	- 7/ 🔍 Attention Patterns of Memory Operations: RMT's attention maps  reveal specific patterns in memory operations during a reasoning task. 💡📚 
	  
	  ![](https://pbs.twimg.com/media/FuQFb3dWYAABiZC.jpg) ([View Tweet](https://twitter.com/MikhailBurtsev/status/1649452167940505617))
	- 8/ 🔗 Report: https://t.co/mRl9iTZlN0
	  Code: https://t.co/fn82VhBpO3 ([View Tweet](https://twitter.com/MikhailBurtsev/status/1649452171950260241))
	- @booydar and @yurakuratov did all the job and I just have a lot of fun! 🥸 ([View Tweet](https://twitter.com/MikhailBurtsev/status/1649477072257949721))