title:: ğŸš€ 1/ Excited to Share Ou... (highlights)
author:: [[@MikhailBurtsev on Twitter]]
full-title:: "ğŸš€ 1/ Excited to Share Ou..."
category:: #tweets
url:: https://twitter.com/MikhailBurtsev/status/1649452141084397569

- Highlights first synced by [[Readwise]] [[Apr 27th, 2023]]
	- ğŸš€ 1/ Excited to share our (with Aydar Bulatov and @yurakuratov ) report on scaling Recurrent Memory Transformer to 2M (yes, two millions)ğŸ˜® tokens!  ğŸ§ ğŸŒ #AI #NLP  #DeepLearning 
	  
	  ![](https://pbs.twimg.com/media/FuQBsX1WYAAS0-s.jpg) ([View Tweet](https://twitter.com/MikhailBurtsev/status/1649452141084397569))
		- **Note**: Thread
	- 2/ ğŸ“ˆ We've tackled the quadratic complexity of attention in  #Transformers by combining token-based memory & segment-level  recurrence, using RMT.
	  ğŸ”¸ RMT adapts to any Transformer family model 
	  ğŸ”¸ Memory tokens provide the recurrent connection ğŸ›ï¸ğŸ’¡ #AI #NLP #DeepLearning 
	  
	  ![](https://pbs.twimg.com/media/FuQC8czXgAgHN7g.png) ([View Tweet](https://twitter.com/MikhailBurtsev/status/1649452145148657684))
	- 3/ ğŸ§  We tested RMT's memorization capabilities with synthetic datasets requiring fact memorization, detection, & reasoning. The model must separate facts from irrelevant text and use them to answer questions in a 6-class classification. ğŸ¯ #AI #NLP #DeepLearning 
	  
	  ![](https://pbs.twimg.com/media/FuQENh2XgAIJNQQ.jpg) ([View Tweet](https://twitter.com/MikhailBurtsev/status/1649452148118155265))
	- 4/ ğŸ“Š In our experiments, we used the pretrained BERT model as the backbone for RMT. We employed curriculum learning, starting with shorter tasks & increasing length upon convergence. This improved accuracy & stability in our model's performance. ğŸ’ª #AI #NLP #DeepLearning 
	  
	  ![](https://pbs.twimg.com/media/FuQEgeTXgAQI7lQ.jpg) ([View Tweet](https://twitter.com/MikhailBurtsev/status/1649452152513875968))
	- 5/ ğŸ“ˆ RMT's extrapolation abilities: Models trained on 7 segments  generalize surprisingly well  even on sequences up to 2,043,904 tokens! ğŸ”ğŸš€ #AI #NLP #DeepLearning 
	  
	  ![](https://pbs.twimg.com/media/FuQE5QrXgA4DLId.jpg) ([View Tweet](https://twitter.com/MikhailBurtsev/status/1649452160617250832))
	- 6/ ğŸƒ Computational efficiency: RMT scales linearly for any model size  with fixed segment length. Larger Transformers exhibit slower quadratic  scaling, but RMT requires fewer FLOPs and can reduce FLOPs by up to  295x! ğŸŒŸâœ‚ï¸ #AI #NLP #DeepLearning #Efficiency 
	  
	  ![](https://pbs.twimg.com/media/FuQFQIQXgA4AtPS.jpg) ([View Tweet](https://twitter.com/MikhailBurtsev/status/1649452164375347208))
	- 7/ ğŸ” Attention Patterns of Memory Operations: RMT's attention maps  reveal specific patterns in memory operations during a reasoning task. ğŸ’¡ğŸ“š 
	  
	  ![](https://pbs.twimg.com/media/FuQFb3dWYAABiZC.jpg) ([View Tweet](https://twitter.com/MikhailBurtsev/status/1649452167940505617))
	- 8/ ğŸ”— Report: https://t.co/mRl9iTZlN0
	  Code: https://t.co/fn82VhBpO3 ([View Tweet](https://twitter.com/MikhailBurtsev/status/1649452171950260241))
	- @booydar and @yurakuratov did all the job and I just have a lot of fun! ğŸ¥¸ ([View Tweet](https://twitter.com/MikhailBurtsev/status/1649477072257949721))