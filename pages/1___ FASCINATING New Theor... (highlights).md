title:: 1/ FASCINATING New Theor... (highlights)
author:: [[@wolfejosh on Twitter]]
full-title:: "1/ FASCINATING New Theor..."
category:: #tweets
url:: https://twitter.com/wolfejosh/status/1520954573552496641

- Highlights first synced by [[Readwise]] [[Nov 19th, 2022]]
	- 1/ FASCINATING new theory on DREAMING from observing a deep neural net
	  
	  but 1st‚Äì‚Äìsome of the most cutting-edge + interesting research  on how üß†üß† work ‚Äì‚Äì are inspired by observation of modern üíªüíª + algorithms
	  
	  every era there's a theory of brains running parallel to era's tech 
	  
	  ![](https://pbs.twimg.com/media/FRuEENJXMAAWoXy.jpg) ([View Tweet](https://twitter.com/wolfejosh/status/1520954573552496641))
		- **Note**: Thread
	- 2/ Go back to [[Descartes]] who thought the üß† worked like hydraulic pumps ‚õΩÔ∏è‚Äì‚Äìthe available new tech or his era 
	  
	  ![](https://pbs.twimg.com/media/FRuEXW6XEAIK9xL.png) ([View Tweet](https://twitter.com/wolfejosh/status/1520954939543011328))
	- 3/ Freud looked to the tech of his time to describe the mechanics of the brain‚Äì‚Äìthe steam engine 
	  
	  ![](https://pbs.twimg.com/media/FRuFJNfXIAEdWZZ.png) ([View Tweet](https://twitter.com/wolfejosh/status/1520955716869128192))
	- 4/ More recent analogies have been to the brain as a computer‚Äì‚Äìwhich notably inspired lots of AI research, specifically the early work on neural nets which lost and regained favor over the decades 
	  
	  ![](https://pbs.twimg.com/media/FRuFjOgXMAAMXpx.jpg) ([View Tweet](https://twitter.com/wolfejosh/status/1520956347378171906))
	- 5/ Then we have had the analogy of the brain as an internet‚Äì‚Äìwith islands of functional groups interconnected 
	  
	  ![](https://pbs.twimg.com/media/FRuF-YdXIAAm5H7.jpg) ([View Tweet](https://twitter.com/wolfejosh/status/1520956703000633344))
	- 6/ All models are wrong‚Äì‚Äìsome of them are useful.
	  
	  Insights from trying to understand our internal human SENSES, PERCEPTION, SPEECH, VISION, HEARING, MEMORY have all led to embodied technologies
	  
	  which in turn lead to new theories...
	  https://t.co/51ZWfbltjJ ([View Tweet](https://twitter.com/wolfejosh/status/1520957575659143168))
	- 7/ We already know we SEE what we BELIEVE 
	  
	  Illusions are excellent at humbling us. 
	  Even if we know they are illusions.
	  https://t.co/nLJxgVUjgu ([View Tweet](https://twitter.com/wolfejosh/status/1520957766663172096))
	- 8/ (almost there...stick with me;) 
	  Now our study + design of neural nets is leading to a 'consilience of inductions'‚Äì‚Äì
	  
	  many different researchers convening on common explanations that point to same conclusion
	  https://t.co/WgatFIpaf3 ([View Tweet](https://twitter.com/wolfejosh/status/1520958117286379522))
	- 9/ Like "memory‚Äì‚Äìprediction" framework and the computational layer between them 
	  
	  that ingests reality, makes models + predictions of patterns it later expects to see, then updates models based on 'reality' (just as robots/machine vision do)
	  https://t.co/iZAxrBfaMg ([View Tweet](https://twitter.com/wolfejosh/status/1520958570350583808))
	- 10/ now ‚Äì‚Äì Erik Hoel has a COOL hypothesis
	  
	  what if the REASON we DREAM‚Äì‚Äìwas similar to
	  the REASON programmers add noise to deep neural nets 
	  
	  to prevent narrow training from experience
	  + generalize, allowing for anticipation of weird new stuff‚Äì‚Äìand be evolutionarily adaptive... 
	  
	  ![](https://pbs.twimg.com/media/FRuIZYeWUAAumal.png) ([View Tweet](https://twitter.com/wolfejosh/status/1520959300105293824))
	- 11/ Hoel calls it the Overfitting Brain Hypothesis 
	  
	  the problem of OVERFITTING in machine learning is best visualized by this 
	  
	  ![](https://pbs.twimg.com/media/FRuItqVX0AQF9xh.jpg) ([View Tweet](https://twitter.com/wolfejosh/status/1520960286689091587))
	- 12/ The way researchers solve the "overfitting" problem for Deep Neural Nets‚Äì‚Äìis by introducing "noise injections" in the form of corrupt inputs
	  
	  Why? So the algorithms don't treat everything so narrowly SPECIFIC and precise‚Äì‚Äìbut instead can better GENERALIZE 
	  
	  ![](https://pbs.twimg.com/media/FRuKD9UWYAA-Pil.jpg) ([View Tweet](https://twitter.com/wolfejosh/status/1520961456354709504))
	- 13/ Now IF our brain processes + stores information from stimulus it receives all day long‚Äì‚Äìand learns from experiences in a narrow way‚Äì‚ÄìTHEN it too can "overfit" a model 
	  
	  & be less fit to encounter wider variations from it 
	  (like the real world)...
	  
	  So the PROVOCATIVE theory... ([View Tweet](https://twitter.com/wolfejosh/status/1520961572860227584))
	- 14/...Is that the evolutionary PURPOSE of DREAMING is to purposely corrupt data (memory or predictions) by inject noise into the system
	  
	  And prevent learning from just rote routine memorization
	  
	  Basically‚Äì‚Äì
	  natural hallucination improves generalization
	  ü§Ø 
	  
	  ![](https://pbs.twimg.com/media/FRuLcqUXIAcPKei.jpg) ([View Tweet](https://twitter.com/wolfejosh/status/1520963214632275969))
	- 15/ Link to full paper PDF here‚Äì‚Äìa quick and VERY provocative read from @erikphoel https://t.co/JHHm2szrHg ([View Tweet](https://twitter.com/wolfejosh/status/1520963602303225856))