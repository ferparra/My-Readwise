title:: Deep Symbolic Regression... (highlights)
author:: [[@GuillaumeLample on Twitter]]
full-title:: "Deep Symbolic Regression..."
category:: #tweets
url:: https://twitter.com/GuillaumeLample/status/1482024620224126977

- Highlights first synced by [[Readwise]] [[Dec 18th, 2022]]
	- Deep Symbolic Regression for Recurrent Sequences -- https://t.co/bL7CUzY85i We show that transformers are great at predicting symbolic functions from values, and can predict the recurrence relation of sequences better than Mathematica. You can try it here: https://t.co/7ptmZhPNyf 
	  
	  ![](https://pbs.twimg.com/media/FJEk-57XEAE9DoY.jpg) 
	  
	  ![](https://pbs.twimg.com/media/FJE0RxQWUAQ36vR.jpg) 
	  
	  ![](https://pbs.twimg.com/media/FJE0zx2XwAEWi56.jpg) ([View Tweet](https://twitter.com/GuillaumeLample/status/1482024620224126977))
		- **Note**: Thread
	- As a surprising by-product, our model is capable of approximating out-of-vocabulary constants and functions with its own building blocks. Feed it with sum(1/n^2), and it will predict pi^2/6. Feed it with bessel0, it will find an asymptotic estimate (sin(x)+cos(x))/sqrt(pi*x) 
	  
	  ![](https://pbs.twimg.com/media/FJE1TH-XsAECjzI.png) ([View Tweet](https://twitter.com/GuillaumeLample/status/1482024626649763840))
	- Check out the paper for more details! Notably, the intriguing structure of embeddings learned by the model
	  With @stephanedascoli @pa_kamienny and @f_charton 
	  
	  ![](https://pbs.twimg.com/media/FJEjp8wWQAA2S5b.jpg) ([View Tweet](https://twitter.com/GuillaumeLample/status/1482024632370794498))