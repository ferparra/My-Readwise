title:: Tweets From Alex Bugeja (highlights)
author:: [[@arbuge on Twitter]]
full-title:: "Tweets From Alex Bugeja"
category:: #tweets
url:: https://twitter.com/arbuge

- Highlights first synced by [[Readwise]] [[Apr 7th, 2023]]
	- @tito @pinecone @LangChainAI Have you tried playing with the text splitter parameters of the vector store?
	  
	  Also, feeding multiple search results into the LLM and telling it in the prompt to prioritize them in the order they were returned from the vector search might help. ([View Tweet](https://twitter.com/arbuge/status/1642937134339309570))