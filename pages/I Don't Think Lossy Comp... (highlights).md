title:: I Don't Think Lossy Comp... (highlights)
author:: [[@raphaelmilliere on Twitter]]
full-title:: "I Don't Think Lossy Comp..."
category:: #tweets
url:: https://twitter.com/raphaelmilliere/status/1624073150475431940

- Highlights first synced by [[Readwise]] [[Feb 12th, 2023]]
	- I don't think lossy compression is a very helpful analogy to convey what (linguistic or multimodal) generative models do – at least if "blurry JPEGs" is the leading metaphor.  It might work in a loose sense, but it doesn't tell the whole story. 1/
	  
	  https://t.co/4sSTC5VkYm ([View Tweet](https://twitter.com/raphaelmilliere/status/1624073150475431940))
		- **Note**: Thread
	- Generative models can definitely be used for lossy compression (see below), but that's a special case of their generative capabilities. Reducing all they do to LC perpetuates the idea that they just regurgitate approximations of their training samples. 2/
	  
	  https://t.co/7ohkUA1yAg ([View Tweet](https://twitter.com/raphaelmilliere/status/1624073152228741122))
	- This bit about interpolation strikes me as particularly misleading. Inference on generative models involves computations that are way more complex and structured than (say) nearest neighbor pixel interpolation in image decompression. 3/ 
	  
	  ![](https://pbs.twimg.com/media/FonW9EmWcAAdqgR.png) ([View Tweet](https://twitter.com/raphaelmilliere/status/1624073153742880770))
	- It's a bit like saying "LLMs are just big look-up tables!".  There's a loose sense in which it's not wrong (they map input sequences to predicted outputs). But that's at best a surface-level description of what they do. It provides a satisfying illusion of understanding it. 4/ ([View Tweet](https://twitter.com/raphaelmilliere/status/1624073155999330304))
	- I absolutely get the imperative to resist anthropomorphism especially in public writing about generative models – and this kind of simple analogy may seem helpful for that. But I also worry about what @sleepinyourhat called the dangers of underclaiming: https://t.co/39Gpo8l4cW 5/ ([View Tweet](https://twitter.com/raphaelmilliere/status/1624073157265981442))
	- Conveying in simple prose the nuances of what generative models can and can't do, what they are and aren't, is a high-wire act even for experts. @mpshanahan offers a very balanced (if slightly deflationary) treatment of this question in this preprint: https://t.co/XnQO8zXjQ0 6/ ([View Tweet](https://twitter.com/raphaelmilliere/status/1624073158801211400))
	- I certainly can't fault Ted Chiang for failing to answer in a short general audience piece questions that experts still grapple with. But I think we should be careful not to take all-encompassing metaphors about generative models, from parrots to compression, too literally. 7/ ([View Tweet](https://twitter.com/raphaelmilliere/status/1624073160302727174))
	- See also this great thread by @AndrewLampinen on why LLMs do much more than lossy memorization: https://t.co/giTG91cqwd ([View Tweet](https://twitter.com/raphaelmilliere/status/1624423555571060741))