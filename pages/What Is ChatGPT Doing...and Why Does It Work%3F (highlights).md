title:: What Is ChatGPT Doing...and Why Does It Work? (highlights)
author:: [[Wolfram]]
full-title:: "What Is ChatGPT Doing...and Why Does It Work?"
category:: #articles
url:: https://www.youtube.com/watch?v=flXrLGPY3SU&list=WL&index=3
document_note:: The author states that the temperature parameter can be used as a way to fix the issue of picking the highest probability word by introducing a certain randomness when picking words. This randomness prevents the model from always picking the most probable word and instead allows for a variety of different words to be chosen that still have the correct probabilities of appearing in the text.

ChatGPT is a neural net whose goal is to continue a piece of text by sampling words from the web and representing them using weights that have been learned by the chat GPT, in order to produce human-like essays.

- Highlights first synced by [[Readwise]] [[Feb 26th, 2023]]
	- what what is chat GPTreally doingbasically the um uh the the kind of theum uh the starting point is it is tryingto write reasonable it is trying to takean initial piece of text that you mightgive and is trying to continue thatpiece of text in a reasonable human-likeway that is sort of characteristic of
	  
	  typical human writing ([View Highlight](https://read.readwise.io/read/01gt5985ngjn427spqaj1rbrh7))
		- **Note**: Chat GPT is trying to do something like a human would do when writing. It takes a piece of text that you give it and tries to continue it in a way that looks like it was written by a person, not a computer.
	- I've already written and it's remarkablethat in the end one can get an essaythat sort of feels like it's coherentand has a structure and so on but reallyin a sense it's being written one wordat a time so let's say that the theprompts have been the best thing aboutAI is its ability too okay what strategygoing to do next well it's uh what it'sgoing to do is it's going to say wellwhat's what what what should the nextword ([View Highlight](https://read.readwise.io/read/01gt59dtzsj1mamqse0jcnx563))
		- **Note**: ChatGPT takes one word prediction at a time.
- New highlights added [[Feb 26th, 2023]] at 10:09 AM
	- chat GPT
	  
	  has learned from it's trying to imitatethe statistics of what it's seen ([View Highlight](https://read.readwise.io/read/01gt5a7x814d5xd34028f19hp3))
	- the problem is there just isn'tenough English text that's ever been ortext of any language that's ever beenwritten to be able to estimate thoseprobabilitiesin this direct way ([View Highlight](https://read.readwise.io/read/01gt5b3exc3k9731414e8hqh9y))
	- there just isn'tenough text that's been written in thefew billion web pages that exist and soon to be able to sample all of those 60trillion triples of words and say what'sthe probability of each one of thesetruffles by the time you get to like a20 word essay uh you you're dealing withthe number of possibilities being morethan the number of particles in theuniverse you wouldn't even be able torecord those probabilities even if youhad text you know written by sort of aninfinite collection ([View Highlight](https://read.readwise.io/read/01gt5b53m8detpv92jhs17pas7))
	- A model issomething where you're kind ofsummarizing data you're summarizingthings in a way where you don't have tohave every piece of data you can makeyou can just have a model which allowsyou to predict more data even if youdidn't immediately have it ([View Highlight](https://read.readwise.io/read/01gt5byk24q20mm85wdce6jx5d))
	- what is a neuralnet it's kind of an idealization of whatwe think is going on in the brain what'sgoing on in the brain what we all haveabout 100 billion neurons in our brainswhich are nerve cells that have the
	  
	  feature that when they get excited theyproduce electrical signals maybe athousand times a secondum they and each nerve cell hasit's it's taking that electrical signaland it's it has sort of wire-likeprojections from the from the nerve cellthat are connecting toum maybe a thousand maybe 10 000 othernerve cells and so what happens in a
	  
	  sort of rough approximation is that uhyou'll have electrical activity in onenerve cell and that will kind of uh getcommunicate itself to other nerve cellsand there's this whole network of nervesthat is has this elaborate pattern ofelectrical con electrical activitysoumand roughly the way it seems to work isthat the extent to which one nerve cell
	  
	  will affect others is determined by uhsort of the the weights associated withthese different connections and so oneconnection might have a very strongpositive effect on another nerve cell ifthe first nerve cell is fired then it'slike it makes it very likely the nextnerve cell will fire all that connectionmight be an inhibitory connection wherethe if one nerve cell fires it makes itvery unlikely for the next nerve cell tofire there's some whole combination of
	  
	  these weights associated with thesedifferent connections between nervecellsso you know what actually happens whenwe're trying to recognize a two in animage for example well the you know thethe light the photons from from the fromthe image fall on the cells at the backof our eye and a retina there's aphotoreceptor cells they convert thatlight into electrical signals theelectrical signalsum end up going through nerves that getto the visual cortex at the back of our
	  
	  headum and uh there's an array of of uh ofnerves that correspond to all thedifferent essentially pixel positions inthe image and then what's happening isthat within our brains there's thissequence of connections they're sort oflayers of neurons that process theelectrical signals that are coming inand eventually we get to the point wherewe kind of form a thought that thatimage that we're seeing in front of usis a two and then we might say it's atwo
	  
	  um but that process of sort of formingthe thought that's what we're talkingabout as kind of this process ofrecognition I was talking about it inthe in the actual neural Nets that wehave in brains but what is being done inall of these models including thingslike chat GPT is an idealization of thatneural net ([View Highlight](https://read.readwise.io/read/01gt5cqvedefm73a83wtdsedtb))
		- **Note**: We all have around 100 billion nerve cells in our brains. Each nerve cell sends electrical signals to other nerve cells. To recognize something, like a two in an image, the light from the image falls on the cells at the back of our eye and is converted into electrical signals. These signals travel through nerves to our brain. In our brain there are layers of neurons that process the electrical signals and eventually form a thought about the image. This is a process of recognition and it is what ChatGPT does, but in an idealized way.
- New highlights added [[Feb 28th, 2023]] at 9:35 PM
	- let's say youhave a neural net and you want to makeit compute a particular function solet's say let's take a very simple caselet's say we have a neural net we justwant it to compute as a function of X wewanted to compute this particularfunction hereokay so let's pick a neural net there'sa there's a neural net without weightslet's fill in random weights in thatneural net for every random collectionof Weights in the neural Nets the neural
	  
	  net will compute something it won't bethe function we want but the law iscompute something it'll always be thecase that when you feed in some value uphere you'll get out some value down hereand these are plots of the function thatyou get by doing thatokay the the big idea is that if you doit the right way and you can give enoughexamples ofum uhum of um uh of what function you are
	  
	  trying to learnum you will be able to progressivelytweak the weights in this neural net sothat eventually you'll get a neural netthat correctly computes this function soagain what we're doing here is this iswe're just describing if this is Xthis is let's say you know G of X downhere this is the value of x up here andthis is a g of X for some function G and
	  
	  that function G that we want is thiskind of uh Square wave type thing herenow in this particular case this neuralnet with these weights is not Computingthe function we wanted it's Computingthis function here but as weprogressively train this neural net wetweak the weights until eventually weget a neural net that actually computesthe function we want this particularcase it took 10 million examples to getto the point where we have the neuralnet that we want
	  
	  Okay so the umhow does this actually work how is thisactually done how does one uh as I saidat the beginning we just had we startedoff the neural Nets where we had randomweights with random weights thisfunction X to G of X with thatparticular choice of Weights is thisthing here which isn't even close towhat we wantedso even if we have when we have examplesof functions examples of results we howdo we go from those to train the neural
	  
	  rats essentially what we're doing is wewe run we say we've got this neural netuh we say let's pick a value of x 0.2for example let's run it through theneural net let's see what value we getokay we get this value here oh we saythat value is not correct based on whatwe were trying to based on the trainingdata that we have based on this functionthat we're trying to we're trying totrain the neural Nets to generate thattraining it isn't the correct result uh
	  
	  it should have been let's say a minusone and it was in fact a 0.7 orsomething okay so then the idea is thatknowing that we got it wrongwe we can measure how much we got itwrong and we can do that for manydifferent samples we can take let's saya thousand examples of this mapping fromvalue X to function G of X that theneural net computes and we can say ofthose thousand examples
	  
	  um how far off were they and we cancompute what's often called the losswhich is take all those values of whatwhat we should have got versus what weactually got and for example take thesum of the squares of the differencesbetween those valuesum and that gives us a sense of if ifall the values were right on that wouldbe zero but in fact it's not zerobecause we didn't actually get the rightanswer without knowing that and so thenwhat we're trying to do is toprogressively reduce that loss we're
	  
	  trying to progressively tweak the neuralnet so that we reduce that loss so forexample this is what it would typicallylook like you you typically have this isthe loss as a function of the number ofexamples you've shown and what you seeis that as you show more and moreexamples the loss progressivelydecreases reflecting the fact that thethe function that's being computed bythe neural net is getting closer to thefunction we actually wanted andeventually the loss is really quitesmall here and then the function isreally computed by the neural net is
	  
	  really close to the one we wanted that'skind of the idea of training a neuralnet we're trying to tweak the weights toreduce the loss to to get to where wewant okay so let's say we've got aneural that's particular form of Weightswe compute the lossthe loss is really bad it's we're prettyfar away how do we arrange toincrementally get closer to the rightanswer well we have to tweak the weightsbut what direction do we tweak theweights in ([View Highlight](https://read.readwise.io/read/01gt8t1wyzf6fgkkpczwkzhn50))
		- **Note**: Let's say we have a neural net and we want it to compute a certain function. We start by filling in the neural net with random weights. This means that for each set of random weights, the neural net will compute something, but it won't be the function we want. However, if we feed in a value, we will always get a value out. 
		  
		  We can plot the function that we get by doing this. The goal is to find the right way to tweak the weights in the neural net so that it computes the function we want. To do this, we give the neural net a lot of examples of what the function should be, and then we gradually tweak the weights until we get the right result. This process is called training the neural net. It takes a lot of examples to get the right result, but eventually the neural net will compute the correct function.