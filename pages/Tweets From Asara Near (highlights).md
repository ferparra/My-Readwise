title:: Tweets From Asara Near (highlights)
author:: [[@nearcyan on Twitter]]
full-title:: "Tweets From Asara Near"
category:: #tweets
url:: https://twitter.com/nearcyan

- Highlights first synced by [[Readwise]] [[Mar 21st, 2023]]
	- Memorizing Transformers
	  
	  extends language models with the ability to memorize the internal representations of past inputs via knn-lookup into an external memory of recent (k, v) pairs, improving performance with memory size of up to 262K tokens
	  
	  arxiv: https://t.co/88UOLixzqE 
	  
	  ![](https://pbs.twimg.com/media/Frr1R0EXwAAiL0n.png) ([View Tweet](https://twitter.com/nearcyan/status/1637891562385317897))